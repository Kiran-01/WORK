{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ML_Project_Human_Activity_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiran-01/WORK/blob/main/ML_Project_Human_Activity_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DV9fwGeQa3b"
      },
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0BfKPUqQa3m"
      },
      "source": [
        "#import test and train set\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCs-UCfDQa3s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ff3e3997-f01d-46de-a197-315043b73793"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tBodyAcc-mean()-X</th>\n",
              "      <th>tBodyAcc-mean()-Y</th>\n",
              "      <th>tBodyAcc-mean()-Z</th>\n",
              "      <th>tBodyAcc-std()-X</th>\n",
              "      <th>tBodyAcc-std()-Y</th>\n",
              "      <th>tBodyAcc-std()-Z</th>\n",
              "      <th>tBodyAcc-mad()-X</th>\n",
              "      <th>tBodyAcc-mad()-Y</th>\n",
              "      <th>tBodyAcc-mad()-Z</th>\n",
              "      <th>tBodyAcc-max()-X</th>\n",
              "      <th>tBodyAcc-max()-Y</th>\n",
              "      <th>tBodyAcc-max()-Z</th>\n",
              "      <th>tBodyAcc-min()-X</th>\n",
              "      <th>tBodyAcc-min()-Y</th>\n",
              "      <th>tBodyAcc-min()-Z</th>\n",
              "      <th>tBodyAcc-sma()</th>\n",
              "      <th>tBodyAcc-energy()-X</th>\n",
              "      <th>tBodyAcc-energy()-Y</th>\n",
              "      <th>tBodyAcc-energy()-Z</th>\n",
              "      <th>tBodyAcc-iqr()-X</th>\n",
              "      <th>tBodyAcc-iqr()-Y</th>\n",
              "      <th>tBodyAcc-iqr()-Z</th>\n",
              "      <th>tBodyAcc-entropy()-X</th>\n",
              "      <th>tBodyAcc-entropy()-Y</th>\n",
              "      <th>tBodyAcc-entropy()-Z</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,4</th>\n",
              "      <th>tBodyAcc-correlation()-X,Y</th>\n",
              "      <th>tBodyAcc-correlation()-X,Z</th>\n",
              "      <th>tBodyAcc-correlation()-Y,Z</th>\n",
              "      <th>...</th>\n",
              "      <th>fBodyBodyAccJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyAccJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyAccJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyAccJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyAccJerkMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroMag-mean()</th>\n",
              "      <th>fBodyBodyGyroMag-std()</th>\n",
              "      <th>fBodyBodyGyroMag-mad()</th>\n",
              "      <th>fBodyBodyGyroMag-max()</th>\n",
              "      <th>fBodyBodyGyroMag-min()</th>\n",
              "      <th>fBodyBodyGyroMag-sma()</th>\n",
              "      <th>fBodyBodyGyroMag-energy()</th>\n",
              "      <th>fBodyBodyGyroMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mean()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-std()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mad()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-max()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-min()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-sma()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-energy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
              "      <th>angle(tBodyAccMean,gravity)</th>\n",
              "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
              "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
              "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
              "      <th>angle(X,gravityMean)</th>\n",
              "      <th>angle(Y,gravityMean)</th>\n",
              "      <th>angle(Z,gravityMean)</th>\n",
              "      <th>subject</th>\n",
              "      <th>Activity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.288585</td>\n",
              "      <td>-0.020294</td>\n",
              "      <td>-0.132905</td>\n",
              "      <td>-0.995279</td>\n",
              "      <td>-0.983111</td>\n",
              "      <td>-0.913526</td>\n",
              "      <td>-0.995112</td>\n",
              "      <td>-0.983185</td>\n",
              "      <td>-0.923527</td>\n",
              "      <td>-0.934724</td>\n",
              "      <td>-0.567378</td>\n",
              "      <td>-0.744413</td>\n",
              "      <td>0.852947</td>\n",
              "      <td>0.685845</td>\n",
              "      <td>0.814263</td>\n",
              "      <td>-0.965523</td>\n",
              "      <td>-0.999945</td>\n",
              "      <td>-0.999863</td>\n",
              "      <td>-0.994612</td>\n",
              "      <td>-0.994231</td>\n",
              "      <td>-0.987614</td>\n",
              "      <td>-0.943220</td>\n",
              "      <td>-0.407747</td>\n",
              "      <td>-0.679338</td>\n",
              "      <td>-0.602122</td>\n",
              "      <td>0.929294</td>\n",
              "      <td>-0.853011</td>\n",
              "      <td>0.359910</td>\n",
              "      <td>-0.058526</td>\n",
              "      <td>0.256892</td>\n",
              "      <td>-0.224848</td>\n",
              "      <td>0.264106</td>\n",
              "      <td>-0.095246</td>\n",
              "      <td>0.278851</td>\n",
              "      <td>-0.465085</td>\n",
              "      <td>0.491936</td>\n",
              "      <td>-0.190884</td>\n",
              "      <td>0.376314</td>\n",
              "      <td>0.435129</td>\n",
              "      <td>0.660790</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.936508</td>\n",
              "      <td>0.346989</td>\n",
              "      <td>-0.516080</td>\n",
              "      <td>-0.802760</td>\n",
              "      <td>-0.980135</td>\n",
              "      <td>-0.961309</td>\n",
              "      <td>-0.973653</td>\n",
              "      <td>-0.952264</td>\n",
              "      <td>-0.989498</td>\n",
              "      <td>-0.980135</td>\n",
              "      <td>-0.999240</td>\n",
              "      <td>-0.992656</td>\n",
              "      <td>-0.701291</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.128989</td>\n",
              "      <td>0.586156</td>\n",
              "      <td>0.374605</td>\n",
              "      <td>-0.991990</td>\n",
              "      <td>-0.990697</td>\n",
              "      <td>-0.989941</td>\n",
              "      <td>-0.992448</td>\n",
              "      <td>-0.991048</td>\n",
              "      <td>-0.991990</td>\n",
              "      <td>-0.999937</td>\n",
              "      <td>-0.990458</td>\n",
              "      <td>-0.871306</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.074323</td>\n",
              "      <td>-0.298676</td>\n",
              "      <td>-0.710304</td>\n",
              "      <td>-0.112754</td>\n",
              "      <td>0.030400</td>\n",
              "      <td>-0.464761</td>\n",
              "      <td>-0.018446</td>\n",
              "      <td>-0.841247</td>\n",
              "      <td>0.179941</td>\n",
              "      <td>-0.058627</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.278419</td>\n",
              "      <td>-0.016411</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>-0.998245</td>\n",
              "      <td>-0.975300</td>\n",
              "      <td>-0.960322</td>\n",
              "      <td>-0.998807</td>\n",
              "      <td>-0.974914</td>\n",
              "      <td>-0.957686</td>\n",
              "      <td>-0.943068</td>\n",
              "      <td>-0.557851</td>\n",
              "      <td>-0.818409</td>\n",
              "      <td>0.849308</td>\n",
              "      <td>0.685845</td>\n",
              "      <td>0.822637</td>\n",
              "      <td>-0.981930</td>\n",
              "      <td>-0.999991</td>\n",
              "      <td>-0.999788</td>\n",
              "      <td>-0.998405</td>\n",
              "      <td>-0.999150</td>\n",
              "      <td>-0.977866</td>\n",
              "      <td>-0.948225</td>\n",
              "      <td>-0.714892</td>\n",
              "      <td>-0.500930</td>\n",
              "      <td>-0.570979</td>\n",
              "      <td>0.611627</td>\n",
              "      <td>-0.329549</td>\n",
              "      <td>0.284213</td>\n",
              "      <td>0.284595</td>\n",
              "      <td>0.115705</td>\n",
              "      <td>-0.090963</td>\n",
              "      <td>0.294310</td>\n",
              "      <td>-0.281211</td>\n",
              "      <td>0.085988</td>\n",
              "      <td>-0.022153</td>\n",
              "      <td>-0.016657</td>\n",
              "      <td>-0.220643</td>\n",
              "      <td>-0.013429</td>\n",
              "      <td>-0.072692</td>\n",
              "      <td>0.579382</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.841270</td>\n",
              "      <td>0.532061</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.900160</td>\n",
              "      <td>-0.988296</td>\n",
              "      <td>-0.983322</td>\n",
              "      <td>-0.982659</td>\n",
              "      <td>-0.986321</td>\n",
              "      <td>-0.991829</td>\n",
              "      <td>-0.988296</td>\n",
              "      <td>-0.999811</td>\n",
              "      <td>-0.993979</td>\n",
              "      <td>-0.720683</td>\n",
              "      <td>-0.948718</td>\n",
              "      <td>-0.271958</td>\n",
              "      <td>-0.336310</td>\n",
              "      <td>-0.720015</td>\n",
              "      <td>-0.995854</td>\n",
              "      <td>-0.996399</td>\n",
              "      <td>-0.995442</td>\n",
              "      <td>-0.996866</td>\n",
              "      <td>-0.994440</td>\n",
              "      <td>-0.995854</td>\n",
              "      <td>-0.999981</td>\n",
              "      <td>-0.994544</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.158075</td>\n",
              "      <td>-0.595051</td>\n",
              "      <td>-0.861499</td>\n",
              "      <td>0.053477</td>\n",
              "      <td>-0.007435</td>\n",
              "      <td>-0.732626</td>\n",
              "      <td>0.703511</td>\n",
              "      <td>-0.844788</td>\n",
              "      <td>0.180289</td>\n",
              "      <td>-0.054317</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.279653</td>\n",
              "      <td>-0.019467</td>\n",
              "      <td>-0.113462</td>\n",
              "      <td>-0.995380</td>\n",
              "      <td>-0.967187</td>\n",
              "      <td>-0.978944</td>\n",
              "      <td>-0.996520</td>\n",
              "      <td>-0.963668</td>\n",
              "      <td>-0.977469</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>-0.557851</td>\n",
              "      <td>-0.818409</td>\n",
              "      <td>0.843609</td>\n",
              "      <td>0.682401</td>\n",
              "      <td>0.839344</td>\n",
              "      <td>-0.983478</td>\n",
              "      <td>-0.999969</td>\n",
              "      <td>-0.999660</td>\n",
              "      <td>-0.999470</td>\n",
              "      <td>-0.997130</td>\n",
              "      <td>-0.964810</td>\n",
              "      <td>-0.974675</td>\n",
              "      <td>-0.592235</td>\n",
              "      <td>-0.485821</td>\n",
              "      <td>-0.570979</td>\n",
              "      <td>0.273025</td>\n",
              "      <td>-0.086309</td>\n",
              "      <td>0.337202</td>\n",
              "      <td>-0.164739</td>\n",
              "      <td>0.017150</td>\n",
              "      <td>-0.074507</td>\n",
              "      <td>0.342256</td>\n",
              "      <td>-0.332564</td>\n",
              "      <td>0.239281</td>\n",
              "      <td>-0.136204</td>\n",
              "      <td>0.173863</td>\n",
              "      <td>-0.299493</td>\n",
              "      <td>-0.124698</td>\n",
              "      <td>-0.181105</td>\n",
              "      <td>0.608900</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>0.660795</td>\n",
              "      <td>-0.724697</td>\n",
              "      <td>-0.928539</td>\n",
              "      <td>-0.989255</td>\n",
              "      <td>-0.986028</td>\n",
              "      <td>-0.984274</td>\n",
              "      <td>-0.990979</td>\n",
              "      <td>-0.995703</td>\n",
              "      <td>-0.989255</td>\n",
              "      <td>-0.999854</td>\n",
              "      <td>-0.993238</td>\n",
              "      <td>-0.736521</td>\n",
              "      <td>-0.794872</td>\n",
              "      <td>-0.212728</td>\n",
              "      <td>-0.535352</td>\n",
              "      <td>-0.871914</td>\n",
              "      <td>-0.995031</td>\n",
              "      <td>-0.995127</td>\n",
              "      <td>-0.994640</td>\n",
              "      <td>-0.996060</td>\n",
              "      <td>-0.995866</td>\n",
              "      <td>-0.995031</td>\n",
              "      <td>-0.999973</td>\n",
              "      <td>-0.993755</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.555556</td>\n",
              "      <td>0.414503</td>\n",
              "      <td>-0.390748</td>\n",
              "      <td>-0.760104</td>\n",
              "      <td>-0.118559</td>\n",
              "      <td>0.177899</td>\n",
              "      <td>0.100699</td>\n",
              "      <td>0.808529</td>\n",
              "      <td>-0.848933</td>\n",
              "      <td>0.180637</td>\n",
              "      <td>-0.049118</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.279174</td>\n",
              "      <td>-0.026201</td>\n",
              "      <td>-0.123283</td>\n",
              "      <td>-0.996091</td>\n",
              "      <td>-0.983403</td>\n",
              "      <td>-0.990675</td>\n",
              "      <td>-0.997099</td>\n",
              "      <td>-0.982750</td>\n",
              "      <td>-0.989302</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>-0.576159</td>\n",
              "      <td>-0.829711</td>\n",
              "      <td>0.843609</td>\n",
              "      <td>0.682401</td>\n",
              "      <td>0.837869</td>\n",
              "      <td>-0.986093</td>\n",
              "      <td>-0.999976</td>\n",
              "      <td>-0.999736</td>\n",
              "      <td>-0.999504</td>\n",
              "      <td>-0.997180</td>\n",
              "      <td>-0.983799</td>\n",
              "      <td>-0.986007</td>\n",
              "      <td>-0.627446</td>\n",
              "      <td>-0.850930</td>\n",
              "      <td>-0.911872</td>\n",
              "      <td>0.061436</td>\n",
              "      <td>0.074840</td>\n",
              "      <td>0.198204</td>\n",
              "      <td>-0.264307</td>\n",
              "      <td>0.072545</td>\n",
              "      <td>-0.155320</td>\n",
              "      <td>0.323154</td>\n",
              "      <td>-0.170813</td>\n",
              "      <td>0.294938</td>\n",
              "      <td>-0.306081</td>\n",
              "      <td>0.482148</td>\n",
              "      <td>-0.470129</td>\n",
              "      <td>-0.305693</td>\n",
              "      <td>-0.362654</td>\n",
              "      <td>0.507459</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.678921</td>\n",
              "      <td>-0.701131</td>\n",
              "      <td>-0.909639</td>\n",
              "      <td>-0.989413</td>\n",
              "      <td>-0.987836</td>\n",
              "      <td>-0.986850</td>\n",
              "      <td>-0.986749</td>\n",
              "      <td>-0.996199</td>\n",
              "      <td>-0.989413</td>\n",
              "      <td>-0.999876</td>\n",
              "      <td>-0.989136</td>\n",
              "      <td>-0.720891</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.035684</td>\n",
              "      <td>-0.230091</td>\n",
              "      <td>-0.511217</td>\n",
              "      <td>-0.995221</td>\n",
              "      <td>-0.995237</td>\n",
              "      <td>-0.995722</td>\n",
              "      <td>-0.995273</td>\n",
              "      <td>-0.995732</td>\n",
              "      <td>-0.995221</td>\n",
              "      <td>-0.999974</td>\n",
              "      <td>-0.995226</td>\n",
              "      <td>-0.955696</td>\n",
              "      <td>-0.936508</td>\n",
              "      <td>0.404573</td>\n",
              "      <td>-0.117290</td>\n",
              "      <td>-0.482845</td>\n",
              "      <td>-0.036788</td>\n",
              "      <td>-0.012892</td>\n",
              "      <td>0.640011</td>\n",
              "      <td>-0.485366</td>\n",
              "      <td>-0.848649</td>\n",
              "      <td>0.181935</td>\n",
              "      <td>-0.047663</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.276629</td>\n",
              "      <td>-0.016570</td>\n",
              "      <td>-0.115362</td>\n",
              "      <td>-0.998139</td>\n",
              "      <td>-0.980817</td>\n",
              "      <td>-0.990482</td>\n",
              "      <td>-0.998321</td>\n",
              "      <td>-0.979672</td>\n",
              "      <td>-0.990441</td>\n",
              "      <td>-0.942469</td>\n",
              "      <td>-0.569174</td>\n",
              "      <td>-0.824705</td>\n",
              "      <td>0.849095</td>\n",
              "      <td>0.683250</td>\n",
              "      <td>0.837869</td>\n",
              "      <td>-0.992653</td>\n",
              "      <td>-0.999991</td>\n",
              "      <td>-0.999856</td>\n",
              "      <td>-0.999757</td>\n",
              "      <td>-0.998004</td>\n",
              "      <td>-0.981232</td>\n",
              "      <td>-0.991325</td>\n",
              "      <td>-0.786553</td>\n",
              "      <td>-0.559477</td>\n",
              "      <td>-0.761434</td>\n",
              "      <td>0.313276</td>\n",
              "      <td>-0.131208</td>\n",
              "      <td>0.191161</td>\n",
              "      <td>0.086904</td>\n",
              "      <td>0.257615</td>\n",
              "      <td>-0.272505</td>\n",
              "      <td>0.434728</td>\n",
              "      <td>-0.315375</td>\n",
              "      <td>0.439744</td>\n",
              "      <td>-0.269069</td>\n",
              "      <td>0.179414</td>\n",
              "      <td>-0.088952</td>\n",
              "      <td>-0.155804</td>\n",
              "      <td>-0.189763</td>\n",
              "      <td>0.599213</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.559058</td>\n",
              "      <td>-0.528901</td>\n",
              "      <td>-0.858933</td>\n",
              "      <td>-0.991433</td>\n",
              "      <td>-0.989059</td>\n",
              "      <td>-0.987744</td>\n",
              "      <td>-0.991462</td>\n",
              "      <td>-0.998353</td>\n",
              "      <td>-0.991433</td>\n",
              "      <td>-0.999902</td>\n",
              "      <td>-0.989321</td>\n",
              "      <td>-0.763372</td>\n",
              "      <td>-0.897436</td>\n",
              "      <td>-0.273582</td>\n",
              "      <td>-0.510282</td>\n",
              "      <td>-0.830702</td>\n",
              "      <td>-0.995093</td>\n",
              "      <td>-0.995465</td>\n",
              "      <td>-0.995279</td>\n",
              "      <td>-0.995609</td>\n",
              "      <td>-0.997418</td>\n",
              "      <td>-0.995093</td>\n",
              "      <td>-0.999974</td>\n",
              "      <td>-0.995487</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.936508</td>\n",
              "      <td>0.087753</td>\n",
              "      <td>-0.351471</td>\n",
              "      <td>-0.699205</td>\n",
              "      <td>0.123320</td>\n",
              "      <td>0.122542</td>\n",
              "      <td>0.693578</td>\n",
              "      <td>-0.615971</td>\n",
              "      <td>-0.847865</td>\n",
              "      <td>0.185151</td>\n",
              "      <td>-0.043892</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 563 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  ...  subject  Activity\n",
              "0           0.288585          -0.020294  ...        1  STANDING\n",
              "1           0.278419          -0.016411  ...        1  STANDING\n",
              "2           0.279653          -0.019467  ...        1  STANDING\n",
              "3           0.279174          -0.026201  ...        1  STANDING\n",
              "4           0.276629          -0.016570  ...        1  STANDING\n",
              "\n",
              "[5 rows x 563 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5zbNEI0Qa38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0776987e-0067-4551-b0e4-d8edc6bfcd6e"
      },
      "source": [
        "train.Activity.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WALKING               209\n",
              "STANDING              179\n",
              "LAYING                164\n",
              "WALKING_UPSTAIRS      159\n",
              "WALKING_DOWNSTAIRS    145\n",
              "SITTING               143\n",
              "Name: Activity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cNfS2KCQa4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109e65e5-8159-4d8e-b21e-551937191cfa"
      },
      "source": [
        "train.shape,test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((999, 563), (999, 563))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSXN9QNfQa4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b05831b-b641-4414-8bf4-3e4b472207a4"
      },
      "source": [
        "test.Activity.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WALKING               185\n",
              "LAYING                183\n",
              "STANDING              178\n",
              "SITTING               170\n",
              "WALKING_UPSTAIRS      149\n",
              "WALKING_DOWNSTAIRS    134\n",
              "Name: Activity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zogCnQxGQa4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5240fd-9e4d-4544-ef0d-6f1fcee85a53"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
              "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
              "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
              "       'tBodyAcc-max()-X',\n",
              "       ...\n",
              "       'fBodyBodyGyroJerkMag-kurtosis()', 'angle(tBodyAccMean,gravity)',\n",
              "       'angle(tBodyAccJerkMean),gravityMean)',\n",
              "       'angle(tBodyGyroMean,gravityMean)',\n",
              "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
              "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'subject', 'Activity'],\n",
              "      dtype='object', length=563)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvO1-_VQa4W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "d025037b-a9b4-46b6-e829-5318d40ce51a"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tBodyAcc-mean()-X</th>\n",
              "      <th>tBodyAcc-mean()-Y</th>\n",
              "      <th>tBodyAcc-mean()-Z</th>\n",
              "      <th>tBodyAcc-std()-X</th>\n",
              "      <th>tBodyAcc-std()-Y</th>\n",
              "      <th>tBodyAcc-std()-Z</th>\n",
              "      <th>tBodyAcc-mad()-X</th>\n",
              "      <th>tBodyAcc-mad()-Y</th>\n",
              "      <th>tBodyAcc-mad()-Z</th>\n",
              "      <th>tBodyAcc-max()-X</th>\n",
              "      <th>tBodyAcc-max()-Y</th>\n",
              "      <th>tBodyAcc-max()-Z</th>\n",
              "      <th>tBodyAcc-min()-X</th>\n",
              "      <th>tBodyAcc-min()-Y</th>\n",
              "      <th>tBodyAcc-min()-Z</th>\n",
              "      <th>tBodyAcc-sma()</th>\n",
              "      <th>tBodyAcc-energy()-X</th>\n",
              "      <th>tBodyAcc-energy()-Y</th>\n",
              "      <th>tBodyAcc-energy()-Z</th>\n",
              "      <th>tBodyAcc-iqr()-X</th>\n",
              "      <th>tBodyAcc-iqr()-Y</th>\n",
              "      <th>tBodyAcc-iqr()-Z</th>\n",
              "      <th>tBodyAcc-entropy()-X</th>\n",
              "      <th>tBodyAcc-entropy()-Y</th>\n",
              "      <th>tBodyAcc-entropy()-Z</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,4</th>\n",
              "      <th>tBodyAcc-correlation()-X,Y</th>\n",
              "      <th>tBodyAcc-correlation()-X,Z</th>\n",
              "      <th>tBodyAcc-correlation()-Y,Z</th>\n",
              "      <th>...</th>\n",
              "      <th>fBodyBodyAccJerkMag-iqr()</th>\n",
              "      <th>fBodyBodyAccJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyAccJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyAccJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyAccJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyAccJerkMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroMag-mean()</th>\n",
              "      <th>fBodyBodyGyroMag-std()</th>\n",
              "      <th>fBodyBodyGyroMag-mad()</th>\n",
              "      <th>fBodyBodyGyroMag-max()</th>\n",
              "      <th>fBodyBodyGyroMag-min()</th>\n",
              "      <th>fBodyBodyGyroMag-sma()</th>\n",
              "      <th>fBodyBodyGyroMag-energy()</th>\n",
              "      <th>fBodyBodyGyroMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mean()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-std()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mad()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-max()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-min()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-sma()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-energy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
              "      <th>angle(tBodyAccMean,gravity)</th>\n",
              "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
              "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
              "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
              "      <th>angle(X,gravityMean)</th>\n",
              "      <th>angle(Y,gravityMean)</th>\n",
              "      <th>angle(Z,gravityMean)</th>\n",
              "      <th>subject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "      <td>999.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.272522</td>\n",
              "      <td>-0.017315</td>\n",
              "      <td>-0.106699</td>\n",
              "      <td>-0.564767</td>\n",
              "      <td>-0.421911</td>\n",
              "      <td>-0.601705</td>\n",
              "      <td>-0.596136</td>\n",
              "      <td>-0.439798</td>\n",
              "      <td>-0.591796</td>\n",
              "      <td>-0.411679</td>\n",
              "      <td>-0.265286</td>\n",
              "      <td>-0.552437</td>\n",
              "      <td>0.470642</td>\n",
              "      <td>0.331208</td>\n",
              "      <td>0.613109</td>\n",
              "      <td>-0.500795</td>\n",
              "      <td>-0.812204</td>\n",
              "      <td>-0.883575</td>\n",
              "      <td>-0.870505</td>\n",
              "      <td>-0.657921</td>\n",
              "      <td>-0.586615</td>\n",
              "      <td>-0.590621</td>\n",
              "      <td>-0.043745</td>\n",
              "      <td>-0.034314</td>\n",
              "      <td>-0.139725</td>\n",
              "      <td>-0.158937</td>\n",
              "      <td>0.171664</td>\n",
              "      <td>-0.082287</td>\n",
              "      <td>0.111498</td>\n",
              "      <td>-0.060017</td>\n",
              "      <td>0.079196</td>\n",
              "      <td>0.133614</td>\n",
              "      <td>-0.023147</td>\n",
              "      <td>-0.043669</td>\n",
              "      <td>0.076549</td>\n",
              "      <td>0.037441</td>\n",
              "      <td>-0.159037</td>\n",
              "      <td>-0.167993</td>\n",
              "      <td>-0.171020</td>\n",
              "      <td>0.178503</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.626053</td>\n",
              "      <td>-0.230645</td>\n",
              "      <td>-0.881246</td>\n",
              "      <td>0.150146</td>\n",
              "      <td>-0.281477</td>\n",
              "      <td>-0.597221</td>\n",
              "      <td>-0.643865</td>\n",
              "      <td>-0.661988</td>\n",
              "      <td>-0.636724</td>\n",
              "      <td>-0.695573</td>\n",
              "      <td>-0.863983</td>\n",
              "      <td>-0.643865</td>\n",
              "      <td>-0.865318</td>\n",
              "      <td>-0.664253</td>\n",
              "      <td>0.030325</td>\n",
              "      <td>-0.886091</td>\n",
              "      <td>0.010680</td>\n",
              "      <td>-0.266710</td>\n",
              "      <td>-0.569804</td>\n",
              "      <td>-0.736023</td>\n",
              "      <td>-0.745827</td>\n",
              "      <td>-0.728910</td>\n",
              "      <td>-0.759083</td>\n",
              "      <td>-0.845126</td>\n",
              "      <td>-0.736023</td>\n",
              "      <td>-0.928640</td>\n",
              "      <td>-0.730587</td>\n",
              "      <td>-0.160702</td>\n",
              "      <td>-0.909719</td>\n",
              "      <td>0.098081</td>\n",
              "      <td>-0.219581</td>\n",
              "      <td>-0.541213</td>\n",
              "      <td>0.015125</td>\n",
              "      <td>0.002728</td>\n",
              "      <td>0.004282</td>\n",
              "      <td>-0.013146</td>\n",
              "      <td>-0.545935</td>\n",
              "      <td>0.058899</td>\n",
              "      <td>-0.033470</td>\n",
              "      <td>2.936937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.070183</td>\n",
              "      <td>0.041918</td>\n",
              "      <td>0.056029</td>\n",
              "      <td>0.428018</td>\n",
              "      <td>0.501715</td>\n",
              "      <td>0.340713</td>\n",
              "      <td>0.400180</td>\n",
              "      <td>0.486153</td>\n",
              "      <td>0.351317</td>\n",
              "      <td>0.530312</td>\n",
              "      <td>0.278337</td>\n",
              "      <td>0.247396</td>\n",
              "      <td>0.364312</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.214613</td>\n",
              "      <td>0.442386</td>\n",
              "      <td>0.232467</td>\n",
              "      <td>0.119448</td>\n",
              "      <td>0.142436</td>\n",
              "      <td>0.345498</td>\n",
              "      <td>0.364867</td>\n",
              "      <td>0.361146</td>\n",
              "      <td>0.468796</td>\n",
              "      <td>0.446882</td>\n",
              "      <td>0.342277</td>\n",
              "      <td>0.320439</td>\n",
              "      <td>0.256860</td>\n",
              "      <td>0.246805</td>\n",
              "      <td>0.218513</td>\n",
              "      <td>0.275835</td>\n",
              "      <td>0.223591</td>\n",
              "      <td>0.214110</td>\n",
              "      <td>0.209987</td>\n",
              "      <td>0.306575</td>\n",
              "      <td>0.234642</td>\n",
              "      <td>0.246800</td>\n",
              "      <td>0.226730</td>\n",
              "      <td>0.316266</td>\n",
              "      <td>0.283954</td>\n",
              "      <td>0.325555</td>\n",
              "      <td>...</td>\n",
              "      <td>0.362316</td>\n",
              "      <td>0.664849</td>\n",
              "      <td>0.187424</td>\n",
              "      <td>0.235677</td>\n",
              "      <td>0.330706</td>\n",
              "      <td>0.319046</td>\n",
              "      <td>0.320484</td>\n",
              "      <td>0.304758</td>\n",
              "      <td>0.323747</td>\n",
              "      <td>0.283941</td>\n",
              "      <td>0.186688</td>\n",
              "      <td>0.320484</td>\n",
              "      <td>0.165207</td>\n",
              "      <td>0.310168</td>\n",
              "      <td>0.608457</td>\n",
              "      <td>0.164764</td>\n",
              "      <td>0.284505</td>\n",
              "      <td>0.323845</td>\n",
              "      <td>0.320589</td>\n",
              "      <td>0.261857</td>\n",
              "      <td>0.248742</td>\n",
              "      <td>0.266038</td>\n",
              "      <td>0.242780</td>\n",
              "      <td>0.198877</td>\n",
              "      <td>0.261857</td>\n",
              "      <td>0.093764</td>\n",
              "      <td>0.270914</td>\n",
              "      <td>0.628067</td>\n",
              "      <td>0.101846</td>\n",
              "      <td>0.239421</td>\n",
              "      <td>0.351241</td>\n",
              "      <td>0.360838</td>\n",
              "      <td>0.349059</td>\n",
              "      <td>0.469766</td>\n",
              "      <td>0.629273</td>\n",
              "      <td>0.481251</td>\n",
              "      <td>0.471809</td>\n",
              "      <td>0.349956</td>\n",
              "      <td>0.168279</td>\n",
              "      <td>1.636577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.361205</td>\n",
              "      <td>-0.684097</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.999300</td>\n",
              "      <td>-0.998359</td>\n",
              "      <td>-0.999454</td>\n",
              "      <td>-0.999407</td>\n",
              "      <td>-0.998077</td>\n",
              "      <td>-0.999808</td>\n",
              "      <td>-0.971348</td>\n",
              "      <td>-0.579196</td>\n",
              "      <td>-0.842539</td>\n",
              "      <td>-0.874404</td>\n",
              "      <td>-0.591106</td>\n",
              "      <td>-0.082803</td>\n",
              "      <td>-0.999108</td>\n",
              "      <td>-0.999997</td>\n",
              "      <td>-0.999993</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.999287</td>\n",
              "      <td>-0.998143</td>\n",
              "      <td>-0.999775</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.956249</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.898102</td>\n",
              "      <td>-0.853011</td>\n",
              "      <td>-0.895521</td>\n",
              "      <td>-0.822053</td>\n",
              "      <td>-0.710933</td>\n",
              "      <td>-0.796295</td>\n",
              "      <td>-0.652060</td>\n",
              "      <td>-0.806725</td>\n",
              "      <td>-0.804012</td>\n",
              "      <td>-0.564186</td>\n",
              "      <td>-0.772543</td>\n",
              "      <td>-0.919446</td>\n",
              "      <td>-0.990843</td>\n",
              "      <td>-0.950335</td>\n",
              "      <td>-0.914463</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.999290</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.728314</td>\n",
              "      <td>-0.913185</td>\n",
              "      <td>-0.998021</td>\n",
              "      <td>-0.999143</td>\n",
              "      <td>-0.999221</td>\n",
              "      <td>-0.999408</td>\n",
              "      <td>-0.999211</td>\n",
              "      <td>-0.999991</td>\n",
              "      <td>-0.999143</td>\n",
              "      <td>-0.999996</td>\n",
              "      <td>-0.999298</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.996242</td>\n",
              "      <td>-0.856475</td>\n",
              "      <td>-0.982836</td>\n",
              "      <td>-0.999572</td>\n",
              "      <td>-0.999560</td>\n",
              "      <td>-0.999513</td>\n",
              "      <td>-0.999825</td>\n",
              "      <td>-0.999798</td>\n",
              "      <td>-0.999572</td>\n",
              "      <td>-0.999999</td>\n",
              "      <td>-0.999515</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.965725</td>\n",
              "      <td>-0.944282</td>\n",
              "      <td>-0.999595</td>\n",
              "      <td>-0.939598</td>\n",
              "      <td>-0.976454</td>\n",
              "      <td>-0.995222</td>\n",
              "      <td>-0.994877</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.875487</td>\n",
              "      <td>-0.980143</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.258468</td>\n",
              "      <td>-0.025925</td>\n",
              "      <td>-0.122726</td>\n",
              "      <td>-0.990822</td>\n",
              "      <td>-0.968894</td>\n",
              "      <td>-0.973927</td>\n",
              "      <td>-0.992207</td>\n",
              "      <td>-0.970928</td>\n",
              "      <td>-0.972885</td>\n",
              "      <td>-0.933391</td>\n",
              "      <td>-0.557851</td>\n",
              "      <td>-0.810247</td>\n",
              "      <td>0.163498</td>\n",
              "      <td>0.036256</td>\n",
              "      <td>0.434970</td>\n",
              "      <td>-0.977870</td>\n",
              "      <td>-0.999906</td>\n",
              "      <td>-0.999658</td>\n",
              "      <td>-0.999159</td>\n",
              "      <td>-0.993512</td>\n",
              "      <td>-0.977752</td>\n",
              "      <td>-0.970998</td>\n",
              "      <td>-0.541931</td>\n",
              "      <td>-0.491138</td>\n",
              "      <td>-0.464866</td>\n",
              "      <td>-0.429795</td>\n",
              "      <td>-0.038481</td>\n",
              "      <td>-0.243067</td>\n",
              "      <td>-0.026289</td>\n",
              "      <td>-0.266222</td>\n",
              "      <td>-0.095745</td>\n",
              "      <td>-0.000892</td>\n",
              "      <td>-0.150936</td>\n",
              "      <td>-0.286763</td>\n",
              "      <td>-0.101028</td>\n",
              "      <td>-0.108113</td>\n",
              "      <td>-0.305938</td>\n",
              "      <td>-0.374020</td>\n",
              "      <td>-0.315412</td>\n",
              "      <td>0.014802</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.985596</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.968254</td>\n",
              "      <td>-0.006502</td>\n",
              "      <td>-0.536928</td>\n",
              "      <td>-0.845469</td>\n",
              "      <td>-0.973910</td>\n",
              "      <td>-0.968622</td>\n",
              "      <td>-0.969783</td>\n",
              "      <td>-0.970655</td>\n",
              "      <td>-0.990472</td>\n",
              "      <td>-0.973910</td>\n",
              "      <td>-0.999329</td>\n",
              "      <td>-0.980725</td>\n",
              "      <td>-0.578415</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.204788</td>\n",
              "      <td>-0.507423</td>\n",
              "      <td>-0.809457</td>\n",
              "      <td>-0.989258</td>\n",
              "      <td>-0.988386</td>\n",
              "      <td>-0.987619</td>\n",
              "      <td>-0.990039</td>\n",
              "      <td>-0.991944</td>\n",
              "      <td>-0.989258</td>\n",
              "      <td>-0.999903</td>\n",
              "      <td>-0.988485</td>\n",
              "      <td>-0.871306</td>\n",
              "      <td>-0.968254</td>\n",
              "      <td>-0.038373</td>\n",
              "      <td>-0.476273</td>\n",
              "      <td>-0.815733</td>\n",
              "      <td>-0.144342</td>\n",
              "      <td>-0.316220</td>\n",
              "      <td>-0.512150</td>\n",
              "      <td>-0.392773</td>\n",
              "      <td>-0.795959</td>\n",
              "      <td>0.030145</td>\n",
              "      <td>-0.103119</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.277054</td>\n",
              "      <td>-0.017185</td>\n",
              "      <td>-0.108829</td>\n",
              "      <td>-0.464909</td>\n",
              "      <td>-0.208523</td>\n",
              "      <td>-0.486962</td>\n",
              "      <td>-0.509215</td>\n",
              "      <td>-0.242165</td>\n",
              "      <td>-0.465163</td>\n",
              "      <td>-0.297181</td>\n",
              "      <td>-0.215837</td>\n",
              "      <td>-0.522911</td>\n",
              "      <td>0.392471</td>\n",
              "      <td>0.240750</td>\n",
              "      <td>0.604231</td>\n",
              "      <td>-0.299312</td>\n",
              "      <td>-0.851317</td>\n",
              "      <td>-0.874379</td>\n",
              "      <td>-0.874166</td>\n",
              "      <td>-0.624144</td>\n",
              "      <td>-0.491518</td>\n",
              "      <td>-0.488023</td>\n",
              "      <td>0.132935</td>\n",
              "      <td>0.198148</td>\n",
              "      <td>-0.036501</td>\n",
              "      <td>-0.218721</td>\n",
              "      <td>0.157429</td>\n",
              "      <td>-0.065451</td>\n",
              "      <td>0.118517</td>\n",
              "      <td>-0.095456</td>\n",
              "      <td>0.071850</td>\n",
              "      <td>0.139987</td>\n",
              "      <td>-0.031626</td>\n",
              "      <td>-0.059430</td>\n",
              "      <td>0.049836</td>\n",
              "      <td>0.049390</td>\n",
              "      <td>-0.160775</td>\n",
              "      <td>-0.199180</td>\n",
              "      <td>-0.169186</td>\n",
              "      <td>0.241960</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.589866</td>\n",
              "      <td>0.084382</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>0.135662</td>\n",
              "      <td>-0.307884</td>\n",
              "      <td>-0.680679</td>\n",
              "      <td>-0.621581</td>\n",
              "      <td>-0.661722</td>\n",
              "      <td>-0.622893</td>\n",
              "      <td>-0.722267</td>\n",
              "      <td>-0.927966</td>\n",
              "      <td>-0.621581</td>\n",
              "      <td>-0.915068</td>\n",
              "      <td>-0.641262</td>\n",
              "      <td>0.378448</td>\n",
              "      <td>-0.948718</td>\n",
              "      <td>0.029260</td>\n",
              "      <td>-0.328987</td>\n",
              "      <td>-0.658435</td>\n",
              "      <td>-0.734231</td>\n",
              "      <td>-0.769704</td>\n",
              "      <td>-0.734832</td>\n",
              "      <td>-0.808740</td>\n",
              "      <td>-0.943906</td>\n",
              "      <td>-0.734231</td>\n",
              "      <td>-0.962665</td>\n",
              "      <td>-0.747832</td>\n",
              "      <td>0.163193</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>0.104017</td>\n",
              "      <td>-0.264434</td>\n",
              "      <td>-0.629299</td>\n",
              "      <td>0.010903</td>\n",
              "      <td>0.017954</td>\n",
              "      <td>0.012891</td>\n",
              "      <td>-0.016025</td>\n",
              "      <td>-0.717300</td>\n",
              "      <td>0.223164</td>\n",
              "      <td>0.030593</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.290635</td>\n",
              "      <td>-0.007523</td>\n",
              "      <td>-0.093717</td>\n",
              "      <td>-0.234604</td>\n",
              "      <td>0.045610</td>\n",
              "      <td>-0.314822</td>\n",
              "      <td>-0.289766</td>\n",
              "      <td>0.012828</td>\n",
              "      <td>-0.302503</td>\n",
              "      <td>-0.008596</td>\n",
              "      <td>-0.042372</td>\n",
              "      <td>-0.347202</td>\n",
              "      <td>0.841992</td>\n",
              "      <td>0.681024</td>\n",
              "      <td>0.834789</td>\n",
              "      <td>-0.097338</td>\n",
              "      <td>-0.705217</td>\n",
              "      <td>-0.786314</td>\n",
              "      <td>-0.783440</td>\n",
              "      <td>-0.387686</td>\n",
              "      <td>-0.266701</td>\n",
              "      <td>-0.305545</td>\n",
              "      <td>0.364785</td>\n",
              "      <td>0.352888</td>\n",
              "      <td>0.143711</td>\n",
              "      <td>0.119302</td>\n",
              "      <td>0.373190</td>\n",
              "      <td>0.096396</td>\n",
              "      <td>0.265960</td>\n",
              "      <td>0.142919</td>\n",
              "      <td>0.236184</td>\n",
              "      <td>0.267467</td>\n",
              "      <td>0.113324</td>\n",
              "      <td>0.197750</td>\n",
              "      <td>0.244298</td>\n",
              "      <td>0.201542</td>\n",
              "      <td>-0.002294</td>\n",
              "      <td>0.013471</td>\n",
              "      <td>-0.033685</td>\n",
              "      <td>0.388804</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.315689</td>\n",
              "      <td>0.417555</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>0.300630</td>\n",
              "      <td>-0.066548</td>\n",
              "      <td>-0.425919</td>\n",
              "      <td>-0.370693</td>\n",
              "      <td>-0.409051</td>\n",
              "      <td>-0.362159</td>\n",
              "      <td>-0.500962</td>\n",
              "      <td>-0.809388</td>\n",
              "      <td>-0.370693</td>\n",
              "      <td>-0.768624</td>\n",
              "      <td>-0.400243</td>\n",
              "      <td>0.576270</td>\n",
              "      <td>-0.846154</td>\n",
              "      <td>0.233968</td>\n",
              "      <td>-0.083447</td>\n",
              "      <td>-0.433327</td>\n",
              "      <td>-0.544185</td>\n",
              "      <td>-0.534269</td>\n",
              "      <td>-0.519097</td>\n",
              "      <td>-0.553053</td>\n",
              "      <td>-0.738487</td>\n",
              "      <td>-0.544185</td>\n",
              "      <td>-0.885567</td>\n",
              "      <td>-0.533535</td>\n",
              "      <td>0.408911</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>0.231567</td>\n",
              "      <td>-0.015262</td>\n",
              "      <td>-0.361251</td>\n",
              "      <td>0.178357</td>\n",
              "      <td>0.332586</td>\n",
              "      <td>0.538985</td>\n",
              "      <td>0.350713</td>\n",
              "      <td>-0.606748</td>\n",
              "      <td>0.281283</td>\n",
              "      <td>0.082679</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.498177</td>\n",
              "      <td>0.324130</td>\n",
              "      <td>0.346658</td>\n",
              "      <td>0.543347</td>\n",
              "      <td>0.532506</td>\n",
              "      <td>0.364114</td>\n",
              "      <td>0.495926</td>\n",
              "      <td>0.502260</td>\n",
              "      <td>0.554965</td>\n",
              "      <td>0.680338</td>\n",
              "      <td>0.801531</td>\n",
              "      <td>0.315453</td>\n",
              "      <td>0.858968</td>\n",
              "      <td>0.754046</td>\n",
              "      <td>0.864717</td>\n",
              "      <td>0.413509</td>\n",
              "      <td>0.189526</td>\n",
              "      <td>0.004366</td>\n",
              "      <td>0.548666</td>\n",
              "      <td>0.435403</td>\n",
              "      <td>0.264807</td>\n",
              "      <td>0.866733</td>\n",
              "      <td>0.759709</td>\n",
              "      <td>0.871803</td>\n",
              "      <td>0.913389</td>\n",
              "      <td>0.929294</td>\n",
              "      <td>0.916679</td>\n",
              "      <td>0.639330</td>\n",
              "      <td>0.697457</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.784370</td>\n",
              "      <td>0.927200</td>\n",
              "      <td>0.567294</td>\n",
              "      <td>0.809189</td>\n",
              "      <td>0.737344</td>\n",
              "      <td>0.731823</td>\n",
              "      <td>0.666989</td>\n",
              "      <td>0.989276</td>\n",
              "      <td>0.981234</td>\n",
              "      <td>0.991345</td>\n",
              "      <td>...</td>\n",
              "      <td>0.872551</td>\n",
              "      <td>0.864519</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.862125</td>\n",
              "      <td>0.828929</td>\n",
              "      <td>0.747224</td>\n",
              "      <td>0.106551</td>\n",
              "      <td>0.409251</td>\n",
              "      <td>0.383844</td>\n",
              "      <td>0.384075</td>\n",
              "      <td>0.692489</td>\n",
              "      <td>0.106551</td>\n",
              "      <td>0.025112</td>\n",
              "      <td>0.029427</td>\n",
              "      <td>0.859648</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.659178</td>\n",
              "      <td>0.930262</td>\n",
              "      <td>0.887735</td>\n",
              "      <td>0.081163</td>\n",
              "      <td>-0.031144</td>\n",
              "      <td>0.151936</td>\n",
              "      <td>-0.029240</td>\n",
              "      <td>0.191742</td>\n",
              "      <td>0.081163</td>\n",
              "      <td>-0.430308</td>\n",
              "      <td>0.282360</td>\n",
              "      <td>0.782608</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.918576</td>\n",
              "      <td>0.989538</td>\n",
              "      <td>0.956845</td>\n",
              "      <td>0.955207</td>\n",
              "      <td>0.998425</td>\n",
              "      <td>0.994519</td>\n",
              "      <td>0.971511</td>\n",
              "      <td>0.799174</td>\n",
              "      <td>0.385117</td>\n",
              "      <td>0.265795</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 562 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       tBodyAcc-mean()-X  tBodyAcc-mean()-Y  ...  angle(Z,gravityMean)     subject\n",
              "count         999.000000         999.000000  ...            999.000000  999.000000\n",
              "mean            0.272522          -0.017315  ...             -0.033470    2.936937\n",
              "std             0.070183           0.041918  ...              0.168279    1.636577\n",
              "min            -0.361205          -0.684097  ...             -0.980143    1.000000\n",
              "25%             0.258468          -0.025925  ...             -0.103119    1.000000\n",
              "50%             0.277054          -0.017185  ...              0.030593    3.000000\n",
              "75%             0.290635          -0.007523  ...              0.082679    5.000000\n",
              "max             0.498177           0.324130  ...              0.265795    6.000000\n",
              "\n",
              "[8 rows x 562 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obw2Hn0vsk2C"
      },
      "source": [
        "df = pd.concat([train,test] ,ignore_index=True, sort  = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O64K76llskxs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "5275d540-8c4f-4760-e84b-ab1fe5e56c76"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tBodyAcc-mean()-X</th>\n",
              "      <th>tBodyAcc-mean()-Y</th>\n",
              "      <th>tBodyAcc-mean()-Z</th>\n",
              "      <th>tBodyAcc-std()-X</th>\n",
              "      <th>tBodyAcc-std()-Y</th>\n",
              "      <th>tBodyAcc-std()-Z</th>\n",
              "      <th>tBodyAcc-mad()-X</th>\n",
              "      <th>tBodyAcc-mad()-Y</th>\n",
              "      <th>tBodyAcc-mad()-Z</th>\n",
              "      <th>tBodyAcc-max()-X</th>\n",
              "      <th>tBodyAcc-max()-Y</th>\n",
              "      <th>tBodyAcc-max()-Z</th>\n",
              "      <th>tBodyAcc-min()-X</th>\n",
              "      <th>tBodyAcc-min()-Y</th>\n",
              "      <th>tBodyAcc-min()-Z</th>\n",
              "      <th>tBodyAcc-sma()</th>\n",
              "      <th>tBodyAcc-energy()-X</th>\n",
              "      <th>tBodyAcc-energy()-Y</th>\n",
              "      <th>tBodyAcc-energy()-Z</th>\n",
              "      <th>tBodyAcc-iqr()-X</th>\n",
              "      <th>tBodyAcc-iqr()-Y</th>\n",
              "      <th>tBodyAcc-iqr()-Z</th>\n",
              "      <th>tBodyAcc-entropy()-X</th>\n",
              "      <th>tBodyAcc-entropy()-Y</th>\n",
              "      <th>tBodyAcc-entropy()-Z</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,4</th>\n",
              "      <th>tBodyAcc-correlation()-X,Y</th>\n",
              "      <th>tBodyAcc-correlation()-X,Z</th>\n",
              "      <th>tBodyAcc-correlation()-Y,Z</th>\n",
              "      <th>...</th>\n",
              "      <th>fBodyBodyAccJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyAccJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyAccJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyAccJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyAccJerkMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroMag-mean()</th>\n",
              "      <th>fBodyBodyGyroMag-std()</th>\n",
              "      <th>fBodyBodyGyroMag-mad()</th>\n",
              "      <th>fBodyBodyGyroMag-max()</th>\n",
              "      <th>fBodyBodyGyroMag-min()</th>\n",
              "      <th>fBodyBodyGyroMag-sma()</th>\n",
              "      <th>fBodyBodyGyroMag-energy()</th>\n",
              "      <th>fBodyBodyGyroMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mean()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-std()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mad()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-max()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-min()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-sma()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-energy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
              "      <th>angle(tBodyAccMean,gravity)</th>\n",
              "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
              "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
              "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
              "      <th>angle(X,gravityMean)</th>\n",
              "      <th>angle(Y,gravityMean)</th>\n",
              "      <th>angle(Z,gravityMean)</th>\n",
              "      <th>subject</th>\n",
              "      <th>Activity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.288585</td>\n",
              "      <td>-0.020294</td>\n",
              "      <td>-0.132905</td>\n",
              "      <td>-0.995279</td>\n",
              "      <td>-0.983111</td>\n",
              "      <td>-0.913526</td>\n",
              "      <td>-0.995112</td>\n",
              "      <td>-0.983185</td>\n",
              "      <td>-0.923527</td>\n",
              "      <td>-0.934724</td>\n",
              "      <td>-0.567378</td>\n",
              "      <td>-0.744413</td>\n",
              "      <td>0.852947</td>\n",
              "      <td>0.685845</td>\n",
              "      <td>0.814263</td>\n",
              "      <td>-0.965523</td>\n",
              "      <td>-0.999945</td>\n",
              "      <td>-0.999863</td>\n",
              "      <td>-0.994612</td>\n",
              "      <td>-0.994231</td>\n",
              "      <td>-0.987614</td>\n",
              "      <td>-0.943220</td>\n",
              "      <td>-0.407747</td>\n",
              "      <td>-0.679338</td>\n",
              "      <td>-0.602122</td>\n",
              "      <td>0.929294</td>\n",
              "      <td>-0.853011</td>\n",
              "      <td>0.359910</td>\n",
              "      <td>-0.058526</td>\n",
              "      <td>0.256892</td>\n",
              "      <td>-0.224848</td>\n",
              "      <td>0.264106</td>\n",
              "      <td>-0.095246</td>\n",
              "      <td>0.278851</td>\n",
              "      <td>-0.465085</td>\n",
              "      <td>0.491936</td>\n",
              "      <td>-0.190884</td>\n",
              "      <td>0.376314</td>\n",
              "      <td>0.435129</td>\n",
              "      <td>0.660790</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.936508</td>\n",
              "      <td>0.346989</td>\n",
              "      <td>-0.516080</td>\n",
              "      <td>-0.802760</td>\n",
              "      <td>-0.980135</td>\n",
              "      <td>-0.961309</td>\n",
              "      <td>-0.973653</td>\n",
              "      <td>-0.952264</td>\n",
              "      <td>-0.989498</td>\n",
              "      <td>-0.980135</td>\n",
              "      <td>-0.999240</td>\n",
              "      <td>-0.992656</td>\n",
              "      <td>-0.701291</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.128989</td>\n",
              "      <td>0.586156</td>\n",
              "      <td>0.374605</td>\n",
              "      <td>-0.991990</td>\n",
              "      <td>-0.990697</td>\n",
              "      <td>-0.989941</td>\n",
              "      <td>-0.992448</td>\n",
              "      <td>-0.991048</td>\n",
              "      <td>-0.991990</td>\n",
              "      <td>-0.999937</td>\n",
              "      <td>-0.990458</td>\n",
              "      <td>-0.871306</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.074323</td>\n",
              "      <td>-0.298676</td>\n",
              "      <td>-0.710304</td>\n",
              "      <td>-0.112754</td>\n",
              "      <td>0.030400</td>\n",
              "      <td>-0.464761</td>\n",
              "      <td>-0.018446</td>\n",
              "      <td>-0.841247</td>\n",
              "      <td>0.179941</td>\n",
              "      <td>-0.058627</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.278419</td>\n",
              "      <td>-0.016411</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>-0.998245</td>\n",
              "      <td>-0.975300</td>\n",
              "      <td>-0.960322</td>\n",
              "      <td>-0.998807</td>\n",
              "      <td>-0.974914</td>\n",
              "      <td>-0.957686</td>\n",
              "      <td>-0.943068</td>\n",
              "      <td>-0.557851</td>\n",
              "      <td>-0.818409</td>\n",
              "      <td>0.849308</td>\n",
              "      <td>0.685845</td>\n",
              "      <td>0.822637</td>\n",
              "      <td>-0.981930</td>\n",
              "      <td>-0.999991</td>\n",
              "      <td>-0.999788</td>\n",
              "      <td>-0.998405</td>\n",
              "      <td>-0.999150</td>\n",
              "      <td>-0.977866</td>\n",
              "      <td>-0.948225</td>\n",
              "      <td>-0.714892</td>\n",
              "      <td>-0.500930</td>\n",
              "      <td>-0.570979</td>\n",
              "      <td>0.611627</td>\n",
              "      <td>-0.329549</td>\n",
              "      <td>0.284213</td>\n",
              "      <td>0.284595</td>\n",
              "      <td>0.115705</td>\n",
              "      <td>-0.090963</td>\n",
              "      <td>0.294310</td>\n",
              "      <td>-0.281211</td>\n",
              "      <td>0.085988</td>\n",
              "      <td>-0.022153</td>\n",
              "      <td>-0.016657</td>\n",
              "      <td>-0.220643</td>\n",
              "      <td>-0.013429</td>\n",
              "      <td>-0.072692</td>\n",
              "      <td>0.579382</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.841270</td>\n",
              "      <td>0.532061</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.900160</td>\n",
              "      <td>-0.988296</td>\n",
              "      <td>-0.983322</td>\n",
              "      <td>-0.982659</td>\n",
              "      <td>-0.986321</td>\n",
              "      <td>-0.991829</td>\n",
              "      <td>-0.988296</td>\n",
              "      <td>-0.999811</td>\n",
              "      <td>-0.993979</td>\n",
              "      <td>-0.720683</td>\n",
              "      <td>-0.948718</td>\n",
              "      <td>-0.271958</td>\n",
              "      <td>-0.336310</td>\n",
              "      <td>-0.720015</td>\n",
              "      <td>-0.995854</td>\n",
              "      <td>-0.996399</td>\n",
              "      <td>-0.995442</td>\n",
              "      <td>-0.996866</td>\n",
              "      <td>-0.994440</td>\n",
              "      <td>-0.995854</td>\n",
              "      <td>-0.999981</td>\n",
              "      <td>-0.994544</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.158075</td>\n",
              "      <td>-0.595051</td>\n",
              "      <td>-0.861499</td>\n",
              "      <td>0.053477</td>\n",
              "      <td>-0.007435</td>\n",
              "      <td>-0.732626</td>\n",
              "      <td>0.703511</td>\n",
              "      <td>-0.844788</td>\n",
              "      <td>0.180289</td>\n",
              "      <td>-0.054317</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.279653</td>\n",
              "      <td>-0.019467</td>\n",
              "      <td>-0.113462</td>\n",
              "      <td>-0.995380</td>\n",
              "      <td>-0.967187</td>\n",
              "      <td>-0.978944</td>\n",
              "      <td>-0.996520</td>\n",
              "      <td>-0.963668</td>\n",
              "      <td>-0.977469</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>-0.557851</td>\n",
              "      <td>-0.818409</td>\n",
              "      <td>0.843609</td>\n",
              "      <td>0.682401</td>\n",
              "      <td>0.839344</td>\n",
              "      <td>-0.983478</td>\n",
              "      <td>-0.999969</td>\n",
              "      <td>-0.999660</td>\n",
              "      <td>-0.999470</td>\n",
              "      <td>-0.997130</td>\n",
              "      <td>-0.964810</td>\n",
              "      <td>-0.974675</td>\n",
              "      <td>-0.592235</td>\n",
              "      <td>-0.485821</td>\n",
              "      <td>-0.570979</td>\n",
              "      <td>0.273025</td>\n",
              "      <td>-0.086309</td>\n",
              "      <td>0.337202</td>\n",
              "      <td>-0.164739</td>\n",
              "      <td>0.017150</td>\n",
              "      <td>-0.074507</td>\n",
              "      <td>0.342256</td>\n",
              "      <td>-0.332564</td>\n",
              "      <td>0.239281</td>\n",
              "      <td>-0.136204</td>\n",
              "      <td>0.173863</td>\n",
              "      <td>-0.299493</td>\n",
              "      <td>-0.124698</td>\n",
              "      <td>-0.181105</td>\n",
              "      <td>0.608900</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>0.660795</td>\n",
              "      <td>-0.724697</td>\n",
              "      <td>-0.928539</td>\n",
              "      <td>-0.989255</td>\n",
              "      <td>-0.986028</td>\n",
              "      <td>-0.984274</td>\n",
              "      <td>-0.990979</td>\n",
              "      <td>-0.995703</td>\n",
              "      <td>-0.989255</td>\n",
              "      <td>-0.999854</td>\n",
              "      <td>-0.993238</td>\n",
              "      <td>-0.736521</td>\n",
              "      <td>-0.794872</td>\n",
              "      <td>-0.212728</td>\n",
              "      <td>-0.535352</td>\n",
              "      <td>-0.871914</td>\n",
              "      <td>-0.995031</td>\n",
              "      <td>-0.995127</td>\n",
              "      <td>-0.994640</td>\n",
              "      <td>-0.996060</td>\n",
              "      <td>-0.995866</td>\n",
              "      <td>-0.995031</td>\n",
              "      <td>-0.999973</td>\n",
              "      <td>-0.993755</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.555556</td>\n",
              "      <td>0.414503</td>\n",
              "      <td>-0.390748</td>\n",
              "      <td>-0.760104</td>\n",
              "      <td>-0.118559</td>\n",
              "      <td>0.177899</td>\n",
              "      <td>0.100699</td>\n",
              "      <td>0.808529</td>\n",
              "      <td>-0.848933</td>\n",
              "      <td>0.180637</td>\n",
              "      <td>-0.049118</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.279174</td>\n",
              "      <td>-0.026201</td>\n",
              "      <td>-0.123283</td>\n",
              "      <td>-0.996091</td>\n",
              "      <td>-0.983403</td>\n",
              "      <td>-0.990675</td>\n",
              "      <td>-0.997099</td>\n",
              "      <td>-0.982750</td>\n",
              "      <td>-0.989302</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>-0.576159</td>\n",
              "      <td>-0.829711</td>\n",
              "      <td>0.843609</td>\n",
              "      <td>0.682401</td>\n",
              "      <td>0.837869</td>\n",
              "      <td>-0.986093</td>\n",
              "      <td>-0.999976</td>\n",
              "      <td>-0.999736</td>\n",
              "      <td>-0.999504</td>\n",
              "      <td>-0.997180</td>\n",
              "      <td>-0.983799</td>\n",
              "      <td>-0.986007</td>\n",
              "      <td>-0.627446</td>\n",
              "      <td>-0.850930</td>\n",
              "      <td>-0.911872</td>\n",
              "      <td>0.061436</td>\n",
              "      <td>0.074840</td>\n",
              "      <td>0.198204</td>\n",
              "      <td>-0.264307</td>\n",
              "      <td>0.072545</td>\n",
              "      <td>-0.155320</td>\n",
              "      <td>0.323154</td>\n",
              "      <td>-0.170813</td>\n",
              "      <td>0.294938</td>\n",
              "      <td>-0.306081</td>\n",
              "      <td>0.482148</td>\n",
              "      <td>-0.470129</td>\n",
              "      <td>-0.305693</td>\n",
              "      <td>-0.362654</td>\n",
              "      <td>0.507459</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.678921</td>\n",
              "      <td>-0.701131</td>\n",
              "      <td>-0.909639</td>\n",
              "      <td>-0.989413</td>\n",
              "      <td>-0.987836</td>\n",
              "      <td>-0.986850</td>\n",
              "      <td>-0.986749</td>\n",
              "      <td>-0.996199</td>\n",
              "      <td>-0.989413</td>\n",
              "      <td>-0.999876</td>\n",
              "      <td>-0.989136</td>\n",
              "      <td>-0.720891</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.035684</td>\n",
              "      <td>-0.230091</td>\n",
              "      <td>-0.511217</td>\n",
              "      <td>-0.995221</td>\n",
              "      <td>-0.995237</td>\n",
              "      <td>-0.995722</td>\n",
              "      <td>-0.995273</td>\n",
              "      <td>-0.995732</td>\n",
              "      <td>-0.995221</td>\n",
              "      <td>-0.999974</td>\n",
              "      <td>-0.995226</td>\n",
              "      <td>-0.955696</td>\n",
              "      <td>-0.936508</td>\n",
              "      <td>0.404573</td>\n",
              "      <td>-0.117290</td>\n",
              "      <td>-0.482845</td>\n",
              "      <td>-0.036788</td>\n",
              "      <td>-0.012892</td>\n",
              "      <td>0.640011</td>\n",
              "      <td>-0.485366</td>\n",
              "      <td>-0.848649</td>\n",
              "      <td>0.181935</td>\n",
              "      <td>-0.047663</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.276629</td>\n",
              "      <td>-0.016570</td>\n",
              "      <td>-0.115362</td>\n",
              "      <td>-0.998139</td>\n",
              "      <td>-0.980817</td>\n",
              "      <td>-0.990482</td>\n",
              "      <td>-0.998321</td>\n",
              "      <td>-0.979672</td>\n",
              "      <td>-0.990441</td>\n",
              "      <td>-0.942469</td>\n",
              "      <td>-0.569174</td>\n",
              "      <td>-0.824705</td>\n",
              "      <td>0.849095</td>\n",
              "      <td>0.683250</td>\n",
              "      <td>0.837869</td>\n",
              "      <td>-0.992653</td>\n",
              "      <td>-0.999991</td>\n",
              "      <td>-0.999856</td>\n",
              "      <td>-0.999757</td>\n",
              "      <td>-0.998004</td>\n",
              "      <td>-0.981232</td>\n",
              "      <td>-0.991325</td>\n",
              "      <td>-0.786553</td>\n",
              "      <td>-0.559477</td>\n",
              "      <td>-0.761434</td>\n",
              "      <td>0.313276</td>\n",
              "      <td>-0.131208</td>\n",
              "      <td>0.191161</td>\n",
              "      <td>0.086904</td>\n",
              "      <td>0.257615</td>\n",
              "      <td>-0.272505</td>\n",
              "      <td>0.434728</td>\n",
              "      <td>-0.315375</td>\n",
              "      <td>0.439744</td>\n",
              "      <td>-0.269069</td>\n",
              "      <td>0.179414</td>\n",
              "      <td>-0.088952</td>\n",
              "      <td>-0.155804</td>\n",
              "      <td>-0.189763</td>\n",
              "      <td>0.599213</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.559058</td>\n",
              "      <td>-0.528901</td>\n",
              "      <td>-0.858933</td>\n",
              "      <td>-0.991433</td>\n",
              "      <td>-0.989059</td>\n",
              "      <td>-0.987744</td>\n",
              "      <td>-0.991462</td>\n",
              "      <td>-0.998353</td>\n",
              "      <td>-0.991433</td>\n",
              "      <td>-0.999902</td>\n",
              "      <td>-0.989321</td>\n",
              "      <td>-0.763372</td>\n",
              "      <td>-0.897436</td>\n",
              "      <td>-0.273582</td>\n",
              "      <td>-0.510282</td>\n",
              "      <td>-0.830702</td>\n",
              "      <td>-0.995093</td>\n",
              "      <td>-0.995465</td>\n",
              "      <td>-0.995279</td>\n",
              "      <td>-0.995609</td>\n",
              "      <td>-0.997418</td>\n",
              "      <td>-0.995093</td>\n",
              "      <td>-0.999974</td>\n",
              "      <td>-0.995487</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.936508</td>\n",
              "      <td>0.087753</td>\n",
              "      <td>-0.351471</td>\n",
              "      <td>-0.699205</td>\n",
              "      <td>0.123320</td>\n",
              "      <td>0.122542</td>\n",
              "      <td>0.693578</td>\n",
              "      <td>-0.615971</td>\n",
              "      <td>-0.847865</td>\n",
              "      <td>0.185151</td>\n",
              "      <td>-0.043892</td>\n",
              "      <td>1</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993</th>\n",
              "      <td>0.347731</td>\n",
              "      <td>-0.032157</td>\n",
              "      <td>-0.098822</td>\n",
              "      <td>-0.156174</td>\n",
              "      <td>0.043034</td>\n",
              "      <td>-0.357678</td>\n",
              "      <td>-0.242008</td>\n",
              "      <td>-0.107938</td>\n",
              "      <td>-0.309664</td>\n",
              "      <td>0.222755</td>\n",
              "      <td>-0.001843</td>\n",
              "      <td>-0.396313</td>\n",
              "      <td>0.019276</td>\n",
              "      <td>-0.234678</td>\n",
              "      <td>0.475238</td>\n",
              "      <td>-0.138700</td>\n",
              "      <td>-0.641831</td>\n",
              "      <td>-0.789313</td>\n",
              "      <td>-0.811620</td>\n",
              "      <td>-0.357584</td>\n",
              "      <td>-0.450129</td>\n",
              "      <td>-0.250692</td>\n",
              "      <td>0.520463</td>\n",
              "      <td>0.254753</td>\n",
              "      <td>0.177349</td>\n",
              "      <td>-0.644674</td>\n",
              "      <td>0.725276</td>\n",
              "      <td>-0.633989</td>\n",
              "      <td>0.460940</td>\n",
              "      <td>0.350410</td>\n",
              "      <td>-0.090859</td>\n",
              "      <td>0.304489</td>\n",
              "      <td>-0.156482</td>\n",
              "      <td>-0.242327</td>\n",
              "      <td>0.348093</td>\n",
              "      <td>-0.283372</td>\n",
              "      <td>-0.058514</td>\n",
              "      <td>0.032118</td>\n",
              "      <td>0.060637</td>\n",
              "      <td>0.258885</td>\n",
              "      <td>...</td>\n",
              "      <td>0.590803</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>-0.337039</td>\n",
              "      <td>-0.213715</td>\n",
              "      <td>-0.663167</td>\n",
              "      <td>-0.382287</td>\n",
              "      <td>-0.482749</td>\n",
              "      <td>-0.399425</td>\n",
              "      <td>-0.574596</td>\n",
              "      <td>-0.824425</td>\n",
              "      <td>-0.382287</td>\n",
              "      <td>-0.804666</td>\n",
              "      <td>-0.316297</td>\n",
              "      <td>0.629070</td>\n",
              "      <td>-0.589744</td>\n",
              "      <td>0.209796</td>\n",
              "      <td>-0.497224</td>\n",
              "      <td>-0.790271</td>\n",
              "      <td>-0.429554</td>\n",
              "      <td>-0.414725</td>\n",
              "      <td>-0.381506</td>\n",
              "      <td>-0.454728</td>\n",
              "      <td>-0.692636</td>\n",
              "      <td>-0.429554</td>\n",
              "      <td>-0.821570</td>\n",
              "      <td>-0.467743</td>\n",
              "      <td>0.538808</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>-0.142396</td>\n",
              "      <td>-0.185845</td>\n",
              "      <td>-0.570264</td>\n",
              "      <td>-0.782822</td>\n",
              "      <td>0.777496</td>\n",
              "      <td>-0.761512</td>\n",
              "      <td>-0.618794</td>\n",
              "      <td>-0.804792</td>\n",
              "      <td>0.230680</td>\n",
              "      <td>-0.008339</td>\n",
              "      <td>10</td>\n",
              "      <td>WALKING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1994</th>\n",
              "      <td>0.220061</td>\n",
              "      <td>-0.015905</td>\n",
              "      <td>-0.126731</td>\n",
              "      <td>-0.198277</td>\n",
              "      <td>-0.032091</td>\n",
              "      <td>-0.446172</td>\n",
              "      <td>-0.279805</td>\n",
              "      <td>-0.150247</td>\n",
              "      <td>-0.399941</td>\n",
              "      <td>0.222755</td>\n",
              "      <td>-0.045168</td>\n",
              "      <td>-0.499061</td>\n",
              "      <td>0.019276</td>\n",
              "      <td>-0.075824</td>\n",
              "      <td>0.568236</td>\n",
              "      <td>-0.191033</td>\n",
              "      <td>-0.676736</td>\n",
              "      <td>-0.818648</td>\n",
              "      <td>-0.858439</td>\n",
              "      <td>-0.407981</td>\n",
              "      <td>-0.467467</td>\n",
              "      <td>-0.289128</td>\n",
              "      <td>0.336928</td>\n",
              "      <td>0.384897</td>\n",
              "      <td>0.120122</td>\n",
              "      <td>-0.513585</td>\n",
              "      <td>0.491649</td>\n",
              "      <td>-0.260364</td>\n",
              "      <td>0.181620</td>\n",
              "      <td>0.496158</td>\n",
              "      <td>-0.268397</td>\n",
              "      <td>0.434354</td>\n",
              "      <td>-0.110774</td>\n",
              "      <td>-0.024137</td>\n",
              "      <td>0.158720</td>\n",
              "      <td>-0.058213</td>\n",
              "      <td>-0.256380</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.025730</td>\n",
              "      <td>0.215416</td>\n",
              "      <td>...</td>\n",
              "      <td>0.481517</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>-0.122580</td>\n",
              "      <td>-0.026438</td>\n",
              "      <td>-0.486020</td>\n",
              "      <td>-0.384779</td>\n",
              "      <td>-0.491122</td>\n",
              "      <td>-0.428719</td>\n",
              "      <td>-0.611991</td>\n",
              "      <td>-0.786566</td>\n",
              "      <td>-0.384779</td>\n",
              "      <td>-0.808642</td>\n",
              "      <td>-0.531477</td>\n",
              "      <td>0.575128</td>\n",
              "      <td>-0.589744</td>\n",
              "      <td>0.151479</td>\n",
              "      <td>-0.518616</td>\n",
              "      <td>-0.824599</td>\n",
              "      <td>-0.484253</td>\n",
              "      <td>-0.457980</td>\n",
              "      <td>-0.446958</td>\n",
              "      <td>-0.518492</td>\n",
              "      <td>-0.528414</td>\n",
              "      <td>-0.484253</td>\n",
              "      <td>-0.850766</td>\n",
              "      <td>-0.500528</td>\n",
              "      <td>0.404252</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>-0.045302</td>\n",
              "      <td>-0.050319</td>\n",
              "      <td>-0.506324</td>\n",
              "      <td>0.717971</td>\n",
              "      <td>0.202447</td>\n",
              "      <td>-0.683921</td>\n",
              "      <td>0.278074</td>\n",
              "      <td>-0.800463</td>\n",
              "      <td>0.234092</td>\n",
              "      <td>-0.006167</td>\n",
              "      <td>10</td>\n",
              "      <td>WALKING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>0.240172</td>\n",
              "      <td>-0.010125</td>\n",
              "      <td>-0.132276</td>\n",
              "      <td>-0.324234</td>\n",
              "      <td>0.086649</td>\n",
              "      <td>-0.445843</td>\n",
              "      <td>-0.388274</td>\n",
              "      <td>-0.006090</td>\n",
              "      <td>-0.378945</td>\n",
              "      <td>-0.041089</td>\n",
              "      <td>-0.045168</td>\n",
              "      <td>-0.499061</td>\n",
              "      <td>0.288172</td>\n",
              "      <td>-0.198677</td>\n",
              "      <td>0.615009</td>\n",
              "      <td>-0.205043</td>\n",
              "      <td>-0.770063</td>\n",
              "      <td>-0.771851</td>\n",
              "      <td>-0.857929</td>\n",
              "      <td>-0.445294</td>\n",
              "      <td>-0.284297</td>\n",
              "      <td>-0.250477</td>\n",
              "      <td>0.432520</td>\n",
              "      <td>0.453749</td>\n",
              "      <td>0.028906</td>\n",
              "      <td>-0.426662</td>\n",
              "      <td>0.313683</td>\n",
              "      <td>-0.017571</td>\n",
              "      <td>0.012270</td>\n",
              "      <td>0.253641</td>\n",
              "      <td>-0.172136</td>\n",
              "      <td>0.458087</td>\n",
              "      <td>-0.254124</td>\n",
              "      <td>0.043956</td>\n",
              "      <td>0.074495</td>\n",
              "      <td>0.091263</td>\n",
              "      <td>-0.382309</td>\n",
              "      <td>-0.141441</td>\n",
              "      <td>0.037843</td>\n",
              "      <td>0.157223</td>\n",
              "      <td>...</td>\n",
              "      <td>0.306145</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>-0.011383</td>\n",
              "      <td>0.046765</td>\n",
              "      <td>-0.437206</td>\n",
              "      <td>-0.462664</td>\n",
              "      <td>-0.521942</td>\n",
              "      <td>-0.425369</td>\n",
              "      <td>-0.636344</td>\n",
              "      <td>-0.957698</td>\n",
              "      <td>-0.462664</td>\n",
              "      <td>-0.842177</td>\n",
              "      <td>-0.349077</td>\n",
              "      <td>0.582456</td>\n",
              "      <td>-0.589744</td>\n",
              "      <td>0.013801</td>\n",
              "      <td>-0.548818</td>\n",
              "      <td>-0.823336</td>\n",
              "      <td>-0.599378</td>\n",
              "      <td>-0.577622</td>\n",
              "      <td>-0.543821</td>\n",
              "      <td>-0.695423</td>\n",
              "      <td>-0.681469</td>\n",
              "      <td>-0.599378</td>\n",
              "      <td>-0.909424</td>\n",
              "      <td>-0.612079</td>\n",
              "      <td>0.334418</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>0.071036</td>\n",
              "      <td>-0.288763</td>\n",
              "      <td>-0.728757</td>\n",
              "      <td>0.534596</td>\n",
              "      <td>-0.692795</td>\n",
              "      <td>-0.497539</td>\n",
              "      <td>-0.012647</td>\n",
              "      <td>-0.801466</td>\n",
              "      <td>0.233206</td>\n",
              "      <td>-0.007254</td>\n",
              "      <td>10</td>\n",
              "      <td>WALKING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>0.326055</td>\n",
              "      <td>0.016412</td>\n",
              "      <td>-0.128606</td>\n",
              "      <td>-0.335340</td>\n",
              "      <td>0.194411</td>\n",
              "      <td>-0.419720</td>\n",
              "      <td>-0.386108</td>\n",
              "      <td>0.060010</td>\n",
              "      <td>-0.372052</td>\n",
              "      <td>-0.184208</td>\n",
              "      <td>0.358879</td>\n",
              "      <td>-0.336303</td>\n",
              "      <td>0.243282</td>\n",
              "      <td>-0.198677</td>\n",
              "      <td>0.507462</td>\n",
              "      <td>-0.178193</td>\n",
              "      <td>-0.777311</td>\n",
              "      <td>-0.722893</td>\n",
              "      <td>-0.844939</td>\n",
              "      <td>-0.403511</td>\n",
              "      <td>-0.329512</td>\n",
              "      <td>-0.268441</td>\n",
              "      <td>0.504691</td>\n",
              "      <td>0.435210</td>\n",
              "      <td>-0.007391</td>\n",
              "      <td>-0.538459</td>\n",
              "      <td>0.564961</td>\n",
              "      <td>-0.469952</td>\n",
              "      <td>0.337003</td>\n",
              "      <td>0.241573</td>\n",
              "      <td>-0.114249</td>\n",
              "      <td>0.397588</td>\n",
              "      <td>-0.165131</td>\n",
              "      <td>0.197378</td>\n",
              "      <td>-0.051477</td>\n",
              "      <td>0.202195</td>\n",
              "      <td>-0.386940</td>\n",
              "      <td>-0.173761</td>\n",
              "      <td>-0.011982</td>\n",
              "      <td>0.255810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.511799</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>0.024209</td>\n",
              "      <td>-0.065954</td>\n",
              "      <td>-0.491242</td>\n",
              "      <td>-0.385848</td>\n",
              "      <td>-0.451944</td>\n",
              "      <td>-0.333654</td>\n",
              "      <td>-0.595932</td>\n",
              "      <td>-0.931913</td>\n",
              "      <td>-0.385848</td>\n",
              "      <td>-0.793459</td>\n",
              "      <td>-0.275771</td>\n",
              "      <td>0.613754</td>\n",
              "      <td>-0.589744</td>\n",
              "      <td>0.047561</td>\n",
              "      <td>-0.584671</td>\n",
              "      <td>-0.862212</td>\n",
              "      <td>-0.560855</td>\n",
              "      <td>-0.523730</td>\n",
              "      <td>-0.494101</td>\n",
              "      <td>-0.624711</td>\n",
              "      <td>-0.872621</td>\n",
              "      <td>-0.560855</td>\n",
              "      <td>-0.888361</td>\n",
              "      <td>-0.532007</td>\n",
              "      <td>0.399784</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>0.123191</td>\n",
              "      <td>-0.198549</td>\n",
              "      <td>-0.641198</td>\n",
              "      <td>-0.182979</td>\n",
              "      <td>-0.049838</td>\n",
              "      <td>-0.143235</td>\n",
              "      <td>0.557818</td>\n",
              "      <td>-0.804601</td>\n",
              "      <td>0.230581</td>\n",
              "      <td>-0.009566</td>\n",
              "      <td>10</td>\n",
              "      <td>WALKING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>0.342461</td>\n",
              "      <td>-0.041919</td>\n",
              "      <td>-0.122903</td>\n",
              "      <td>-0.331123</td>\n",
              "      <td>0.236781</td>\n",
              "      <td>-0.378248</td>\n",
              "      <td>-0.368269</td>\n",
              "      <td>0.096951</td>\n",
              "      <td>-0.355019</td>\n",
              "      <td>-0.184208</td>\n",
              "      <td>0.358879</td>\n",
              "      <td>-0.220392</td>\n",
              "      <td>0.236401</td>\n",
              "      <td>-0.187806</td>\n",
              "      <td>0.504907</td>\n",
              "      <td>-0.167731</td>\n",
              "      <td>-0.774126</td>\n",
              "      <td>-0.704121</td>\n",
              "      <td>-0.823013</td>\n",
              "      <td>-0.413074</td>\n",
              "      <td>-0.420409</td>\n",
              "      <td>-0.295669</td>\n",
              "      <td>0.562712</td>\n",
              "      <td>0.382143</td>\n",
              "      <td>0.074988</td>\n",
              "      <td>-0.250506</td>\n",
              "      <td>0.204216</td>\n",
              "      <td>-0.081499</td>\n",
              "      <td>0.139626</td>\n",
              "      <td>-0.085867</td>\n",
              "      <td>0.200282</td>\n",
              "      <td>0.110959</td>\n",
              "      <td>-0.063532</td>\n",
              "      <td>0.162505</td>\n",
              "      <td>0.049881</td>\n",
              "      <td>0.090344</td>\n",
              "      <td>-0.370222</td>\n",
              "      <td>-0.193141</td>\n",
              "      <td>-0.054802</td>\n",
              "      <td>0.447092</td>\n",
              "      <td>...</td>\n",
              "      <td>0.618972</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>0.064212</td>\n",
              "      <td>-0.116112</td>\n",
              "      <td>-0.518137</td>\n",
              "      <td>-0.390091</td>\n",
              "      <td>-0.488943</td>\n",
              "      <td>-0.336482</td>\n",
              "      <td>-0.690079</td>\n",
              "      <td>-0.740906</td>\n",
              "      <td>-0.390091</td>\n",
              "      <td>-0.809414</td>\n",
              "      <td>-0.354161</td>\n",
              "      <td>0.580199</td>\n",
              "      <td>-0.589744</td>\n",
              "      <td>0.077147</td>\n",
              "      <td>-0.725953</td>\n",
              "      <td>-0.955061</td>\n",
              "      <td>-0.509305</td>\n",
              "      <td>-0.487531</td>\n",
              "      <td>-0.434466</td>\n",
              "      <td>-0.582191</td>\n",
              "      <td>-0.768041</td>\n",
              "      <td>-0.509305</td>\n",
              "      <td>-0.865620</td>\n",
              "      <td>-0.548202</td>\n",
              "      <td>0.455922</td>\n",
              "      <td>-0.904762</td>\n",
              "      <td>-0.032990</td>\n",
              "      <td>-0.320528</td>\n",
              "      <td>-0.733443</td>\n",
              "      <td>-0.584016</td>\n",
              "      <td>0.181989</td>\n",
              "      <td>0.425170</td>\n",
              "      <td>0.785471</td>\n",
              "      <td>-0.812929</td>\n",
              "      <td>0.224753</td>\n",
              "      <td>-0.009409</td>\n",
              "      <td>10</td>\n",
              "      <td>WALKING</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1998 rows × 563 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      tBodyAcc-mean()-X  tBodyAcc-mean()-Y  ...  subject  Activity\n",
              "0              0.288585          -0.020294  ...        1  STANDING\n",
              "1              0.278419          -0.016411  ...        1  STANDING\n",
              "2              0.279653          -0.019467  ...        1  STANDING\n",
              "3              0.279174          -0.026201  ...        1  STANDING\n",
              "4              0.276629          -0.016570  ...        1  STANDING\n",
              "...                 ...                ...  ...      ...       ...\n",
              "1993           0.347731          -0.032157  ...       10   WALKING\n",
              "1994           0.220061          -0.015905  ...       10   WALKING\n",
              "1995           0.240172          -0.010125  ...       10   WALKING\n",
              "1996           0.326055           0.016412  ...       10   WALKING\n",
              "1997           0.342461          -0.041919  ...       10   WALKING\n",
              "\n",
              "[1998 rows x 563 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rGNDO6bvxtK"
      },
      "source": [
        "#Data Mapping\n",
        "X = df.drop('Activity',axis = 1)\n",
        "y = df['Activity']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoSxnTr7vgQ0"
      },
      "source": [
        "#Train_Test_Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgxzhu1tQa4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTRE-OFLQa4u"
      },
      "source": [
        "#classification models:\n",
        "#NN using MLP\n",
        "#Logistic regression\n",
        "#Random forest classifier\n",
        "#KNN\n",
        "#Decision Tree\n",
        "#Grid search cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyyg31SkQa4z"
      },
      "source": [
        "# Applying supervised neural network using multi-layer-preceptron\n",
        "import sklearn.neural_network as nn\n",
        "mlpSGD  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
        "                        , max_iter=1000 , alpha=1e-4  \\\n",
        "                        , solver='sgd' , verbose=10   \\\n",
        "                        , tol=1e-19 , random_state=1  \\\n",
        "                        , learning_rate_init=.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daogKwEoQa43"
      },
      "source": [
        "mlpADAM  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
        "                        , max_iter=1000 , alpha=1e-4  \\\n",
        "                        , solver='adam' , verbose=10   \\\n",
        "                        , tol=1e-19 , random_state=1  \\\n",
        "                        , learning_rate_init=.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOpCLEKLQa49"
      },
      "source": [
        "mlpLBFGS  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
        "                        , max_iter=1000 , alpha=1e-4  \\\n",
        "                        , solver='lbfgs' , verbose=10   \\\n",
        "                        , tol=1e-19 , random_state=1  \\\n",
        "                        , learning_rate_init=.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUD_oOEFQa5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0aa2bb-1836-4ddf-b797-d22e1d14b8f6"
      },
      "source": [
        "nnModelSGD= mlpSGD.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.07748858\n",
            "Iteration 2, loss = 1.71160641\n",
            "Iteration 3, loss = 1.46699224\n",
            "Iteration 4, loss = 1.29887304\n",
            "Iteration 5, loss = 1.18822436\n",
            "Iteration 6, loss = 1.10107174\n",
            "Iteration 7, loss = 1.02875356\n",
            "Iteration 8, loss = 0.97007502\n",
            "Iteration 9, loss = 0.92105390\n",
            "Iteration 10, loss = 0.87677713\n",
            "Iteration 11, loss = 0.83835084\n",
            "Iteration 12, loss = 0.80408874\n",
            "Iteration 13, loss = 0.77397064\n",
            "Iteration 14, loss = 0.74446516\n",
            "Iteration 15, loss = 0.71913492\n",
            "Iteration 16, loss = 0.69389588\n",
            "Iteration 17, loss = 0.67208796\n",
            "Iteration 18, loss = 0.65031619\n",
            "Iteration 19, loss = 0.63133426\n",
            "Iteration 20, loss = 0.61306312\n",
            "Iteration 21, loss = 0.59752161\n",
            "Iteration 22, loss = 0.57955948\n",
            "Iteration 23, loss = 0.56461627\n",
            "Iteration 24, loss = 0.54951940\n",
            "Iteration 25, loss = 0.53692092\n",
            "Iteration 26, loss = 0.52406729\n",
            "Iteration 27, loss = 0.51123939\n",
            "Iteration 28, loss = 0.50042623\n",
            "Iteration 29, loss = 0.48847930\n",
            "Iteration 30, loss = 0.47846251\n",
            "Iteration 31, loss = 0.46794149\n",
            "Iteration 32, loss = 0.45824025\n",
            "Iteration 33, loss = 0.44926935\n",
            "Iteration 34, loss = 0.44072299\n",
            "Iteration 35, loss = 0.43277990\n",
            "Iteration 36, loss = 0.42398585\n",
            "Iteration 37, loss = 0.41675546\n",
            "Iteration 38, loss = 0.40912621\n",
            "Iteration 39, loss = 0.40288140\n",
            "Iteration 40, loss = 0.39651187\n",
            "Iteration 41, loss = 0.38850575\n",
            "Iteration 42, loss = 0.38212410\n",
            "Iteration 43, loss = 0.37584580\n",
            "Iteration 44, loss = 0.37066177\n",
            "Iteration 45, loss = 0.36458651\n",
            "Iteration 46, loss = 0.35916581\n",
            "Iteration 47, loss = 0.35367434\n",
            "Iteration 48, loss = 0.34836051\n",
            "Iteration 49, loss = 0.34371193\n",
            "Iteration 50, loss = 0.33850254\n",
            "Iteration 51, loss = 0.33428115\n",
            "Iteration 52, loss = 0.32950547\n",
            "Iteration 53, loss = 0.32550033\n",
            "Iteration 54, loss = 0.32112494\n",
            "Iteration 55, loss = 0.31670250\n",
            "Iteration 56, loss = 0.31303757\n",
            "Iteration 57, loss = 0.30848161\n",
            "Iteration 58, loss = 0.30500021\n",
            "Iteration 59, loss = 0.30116261\n",
            "Iteration 60, loss = 0.29765605\n",
            "Iteration 61, loss = 0.29429066\n",
            "Iteration 62, loss = 0.29107390\n",
            "Iteration 63, loss = 0.28738579\n",
            "Iteration 64, loss = 0.28480136\n",
            "Iteration 65, loss = 0.28128522\n",
            "Iteration 66, loss = 0.27852999\n",
            "Iteration 67, loss = 0.27529878\n",
            "Iteration 68, loss = 0.27204196\n",
            "Iteration 69, loss = 0.26931686\n",
            "Iteration 70, loss = 0.26633139\n",
            "Iteration 71, loss = 0.26405112\n",
            "Iteration 72, loss = 0.26109888\n",
            "Iteration 73, loss = 0.25846718\n",
            "Iteration 74, loss = 0.25612769\n",
            "Iteration 75, loss = 0.25311071\n",
            "Iteration 76, loss = 0.25201356\n",
            "Iteration 77, loss = 0.24927614\n",
            "Iteration 78, loss = 0.24623701\n",
            "Iteration 79, loss = 0.24452512\n",
            "Iteration 80, loss = 0.24268950\n",
            "Iteration 81, loss = 0.23942529\n",
            "Iteration 82, loss = 0.23826387\n",
            "Iteration 83, loss = 0.23552613\n",
            "Iteration 84, loss = 0.23329594\n",
            "Iteration 85, loss = 0.23076634\n",
            "Iteration 86, loss = 0.22890887\n",
            "Iteration 87, loss = 0.22692868\n",
            "Iteration 88, loss = 0.22543914\n",
            "Iteration 89, loss = 0.22291378\n",
            "Iteration 90, loss = 0.22157537\n",
            "Iteration 91, loss = 0.21926680\n",
            "Iteration 92, loss = 0.21761020\n",
            "Iteration 93, loss = 0.21579854\n",
            "Iteration 94, loss = 0.21423399\n",
            "Iteration 95, loss = 0.21241900\n",
            "Iteration 96, loss = 0.21054399\n",
            "Iteration 97, loss = 0.20910785\n",
            "Iteration 98, loss = 0.20720779\n",
            "Iteration 99, loss = 0.20540149\n",
            "Iteration 100, loss = 0.20429697\n",
            "Iteration 101, loss = 0.20231444\n",
            "Iteration 102, loss = 0.20135894\n",
            "Iteration 103, loss = 0.19950279\n",
            "Iteration 104, loss = 0.19786787\n",
            "Iteration 105, loss = 0.19657566\n",
            "Iteration 106, loss = 0.19562255\n",
            "Iteration 107, loss = 0.19351147\n",
            "Iteration 108, loss = 0.19219275\n",
            "Iteration 109, loss = 0.19156172\n",
            "Iteration 110, loss = 0.19065066\n",
            "Iteration 111, loss = 0.18833156\n",
            "Iteration 112, loss = 0.18717992\n",
            "Iteration 113, loss = 0.18654212\n",
            "Iteration 114, loss = 0.18432016\n",
            "Iteration 115, loss = 0.18352580\n",
            "Iteration 116, loss = 0.18196346\n",
            "Iteration 117, loss = 0.18077309\n",
            "Iteration 118, loss = 0.17972633\n",
            "Iteration 119, loss = 0.17831430\n",
            "Iteration 120, loss = 0.17719814\n",
            "Iteration 121, loss = 0.17602549\n",
            "Iteration 122, loss = 0.17534716\n",
            "Iteration 123, loss = 0.17394086\n",
            "Iteration 124, loss = 0.17285224\n",
            "Iteration 125, loss = 0.17226322\n",
            "Iteration 126, loss = 0.17042505\n",
            "Iteration 127, loss = 0.16962007\n",
            "Iteration 128, loss = 0.16839057\n",
            "Iteration 129, loss = 0.16740120\n",
            "Iteration 130, loss = 0.16666504\n",
            "Iteration 131, loss = 0.16598441\n",
            "Iteration 132, loss = 0.16442872\n",
            "Iteration 133, loss = 0.16349581\n",
            "Iteration 134, loss = 0.16256033\n",
            "Iteration 135, loss = 0.16158230\n",
            "Iteration 136, loss = 0.16076301\n",
            "Iteration 137, loss = 0.15988458\n",
            "Iteration 138, loss = 0.15886991\n",
            "Iteration 139, loss = 0.15841969\n",
            "Iteration 140, loss = 0.15706429\n",
            "Iteration 141, loss = 0.15658605\n",
            "Iteration 142, loss = 0.15619332\n",
            "Iteration 143, loss = 0.15434175\n",
            "Iteration 144, loss = 0.15343971\n",
            "Iteration 145, loss = 0.15263255\n",
            "Iteration 146, loss = 0.15175425\n",
            "Iteration 147, loss = 0.15094640\n",
            "Iteration 148, loss = 0.15017661\n",
            "Iteration 149, loss = 0.14978690\n",
            "Iteration 150, loss = 0.14848519\n",
            "Iteration 151, loss = 0.14827502\n",
            "Iteration 152, loss = 0.14760278\n",
            "Iteration 153, loss = 0.14670124\n",
            "Iteration 154, loss = 0.14554107\n",
            "Iteration 155, loss = 0.14473948\n",
            "Iteration 156, loss = 0.14397208\n",
            "Iteration 157, loss = 0.14318882\n",
            "Iteration 158, loss = 0.14235838\n",
            "Iteration 159, loss = 0.14179366\n",
            "Iteration 160, loss = 0.14093269\n",
            "Iteration 161, loss = 0.14035591\n",
            "Iteration 162, loss = 0.13950165\n",
            "Iteration 163, loss = 0.13865720\n",
            "Iteration 164, loss = 0.13819232\n",
            "Iteration 165, loss = 0.13740095\n",
            "Iteration 166, loss = 0.13689885\n",
            "Iteration 167, loss = 0.13630588\n",
            "Iteration 168, loss = 0.13616476\n",
            "Iteration 169, loss = 0.13486503\n",
            "Iteration 170, loss = 0.13433990\n",
            "Iteration 171, loss = 0.13375470\n",
            "Iteration 172, loss = 0.13277409\n",
            "Iteration 173, loss = 0.13222560\n",
            "Iteration 174, loss = 0.13164009\n",
            "Iteration 175, loss = 0.13095983\n",
            "Iteration 176, loss = 0.13083668\n",
            "Iteration 177, loss = 0.12993739\n",
            "Iteration 178, loss = 0.12928967\n",
            "Iteration 179, loss = 0.12925566\n",
            "Iteration 180, loss = 0.12803139\n",
            "Iteration 181, loss = 0.12735298\n",
            "Iteration 182, loss = 0.12665774\n",
            "Iteration 183, loss = 0.12626371\n",
            "Iteration 184, loss = 0.12563516\n",
            "Iteration 185, loss = 0.12486058\n",
            "Iteration 186, loss = 0.12431279\n",
            "Iteration 187, loss = 0.12384421\n",
            "Iteration 188, loss = 0.12371968\n",
            "Iteration 189, loss = 0.12279417\n",
            "Iteration 190, loss = 0.12247546\n",
            "Iteration 191, loss = 0.12243163\n",
            "Iteration 192, loss = 0.12135067\n",
            "Iteration 193, loss = 0.12096194\n",
            "Iteration 194, loss = 0.11998290\n",
            "Iteration 195, loss = 0.11967096\n",
            "Iteration 196, loss = 0.11934185\n",
            "Iteration 197, loss = 0.11879204\n",
            "Iteration 198, loss = 0.11831574\n",
            "Iteration 199, loss = 0.11783044\n",
            "Iteration 200, loss = 0.11699596\n",
            "Iteration 201, loss = 0.11674732\n",
            "Iteration 202, loss = 0.11647696\n",
            "Iteration 203, loss = 0.11561221\n",
            "Iteration 204, loss = 0.11562641\n",
            "Iteration 205, loss = 0.11551290\n",
            "Iteration 206, loss = 0.11423230\n",
            "Iteration 207, loss = 0.11380163\n",
            "Iteration 208, loss = 0.11351789\n",
            "Iteration 209, loss = 0.11283441\n",
            "Iteration 210, loss = 0.11321298\n",
            "Iteration 211, loss = 0.11199838\n",
            "Iteration 212, loss = 0.11147304\n",
            "Iteration 213, loss = 0.11110930\n",
            "Iteration 214, loss = 0.11091481\n",
            "Iteration 215, loss = 0.11012991\n",
            "Iteration 216, loss = 0.10982571\n",
            "Iteration 217, loss = 0.10951803\n",
            "Iteration 218, loss = 0.10942237\n",
            "Iteration 219, loss = 0.10857450\n",
            "Iteration 220, loss = 0.10801199\n",
            "Iteration 221, loss = 0.10805612\n",
            "Iteration 222, loss = 0.10728624\n",
            "Iteration 223, loss = 0.10658050\n",
            "Iteration 224, loss = 0.10620131\n",
            "Iteration 225, loss = 0.10624302\n",
            "Iteration 226, loss = 0.10682951\n",
            "Iteration 227, loss = 0.10668727\n",
            "Iteration 228, loss = 0.10515534\n",
            "Iteration 229, loss = 0.10440472\n",
            "Iteration 230, loss = 0.10389996\n",
            "Iteration 231, loss = 0.10364348\n",
            "Iteration 232, loss = 0.10320150\n",
            "Iteration 233, loss = 0.10342697\n",
            "Iteration 234, loss = 0.10281939\n",
            "Iteration 235, loss = 0.10213184\n",
            "Iteration 236, loss = 0.10189530\n",
            "Iteration 237, loss = 0.10128703\n",
            "Iteration 238, loss = 0.10104402\n",
            "Iteration 239, loss = 0.10062143\n",
            "Iteration 240, loss = 0.10010555\n",
            "Iteration 241, loss = 0.09977936\n",
            "Iteration 242, loss = 0.09983291\n",
            "Iteration 243, loss = 0.09971387\n",
            "Iteration 244, loss = 0.09883689\n",
            "Iteration 245, loss = 0.09861175\n",
            "Iteration 246, loss = 0.09811071\n",
            "Iteration 247, loss = 0.09788079\n",
            "Iteration 248, loss = 0.09743703\n",
            "Iteration 249, loss = 0.09714403\n",
            "Iteration 250, loss = 0.09669253\n",
            "Iteration 251, loss = 0.09658096\n",
            "Iteration 252, loss = 0.09627071\n",
            "Iteration 253, loss = 0.09593750\n",
            "Iteration 254, loss = 0.09530559\n",
            "Iteration 255, loss = 0.09556354\n",
            "Iteration 256, loss = 0.09472523\n",
            "Iteration 257, loss = 0.09440530\n",
            "Iteration 258, loss = 0.09431716\n",
            "Iteration 259, loss = 0.09372090\n",
            "Iteration 260, loss = 0.09342371\n",
            "Iteration 261, loss = 0.09312106\n",
            "Iteration 262, loss = 0.09297070\n",
            "Iteration 263, loss = 0.09361863\n",
            "Iteration 264, loss = 0.09254338\n",
            "Iteration 265, loss = 0.09226610\n",
            "Iteration 266, loss = 0.09199164\n",
            "Iteration 267, loss = 0.09158771\n",
            "Iteration 268, loss = 0.09134213\n",
            "Iteration 269, loss = 0.09045951\n",
            "Iteration 270, loss = 0.09069390\n",
            "Iteration 271, loss = 0.09011326\n",
            "Iteration 272, loss = 0.08994310\n",
            "Iteration 273, loss = 0.08945938\n",
            "Iteration 274, loss = 0.08966819\n",
            "Iteration 275, loss = 0.08907419\n",
            "Iteration 276, loss = 0.08855393\n",
            "Iteration 277, loss = 0.08836616\n",
            "Iteration 278, loss = 0.08828809\n",
            "Iteration 279, loss = 0.08781308\n",
            "Iteration 280, loss = 0.08753460\n",
            "Iteration 281, loss = 0.08717976\n",
            "Iteration 282, loss = 0.08693245\n",
            "Iteration 283, loss = 0.08665762\n",
            "Iteration 284, loss = 0.08632052\n",
            "Iteration 285, loss = 0.08629798\n",
            "Iteration 286, loss = 0.08626991\n",
            "Iteration 287, loss = 0.08550795\n",
            "Iteration 288, loss = 0.08543435\n",
            "Iteration 289, loss = 0.08567859\n",
            "Iteration 290, loss = 0.08482268\n",
            "Iteration 291, loss = 0.08499084\n",
            "Iteration 292, loss = 0.08464966\n",
            "Iteration 293, loss = 0.08387606\n",
            "Iteration 294, loss = 0.08441221\n",
            "Iteration 295, loss = 0.08374284\n",
            "Iteration 296, loss = 0.08335698\n",
            "Iteration 297, loss = 0.08303152\n",
            "Iteration 298, loss = 0.08275355\n",
            "Iteration 299, loss = 0.08239754\n",
            "Iteration 300, loss = 0.08236698\n",
            "Iteration 301, loss = 0.08208400\n",
            "Iteration 302, loss = 0.08187476\n",
            "Iteration 303, loss = 0.08181627\n",
            "Iteration 304, loss = 0.08128932\n",
            "Iteration 305, loss = 0.08108050\n",
            "Iteration 306, loss = 0.08080090\n",
            "Iteration 307, loss = 0.08066882\n",
            "Iteration 308, loss = 0.08035169\n",
            "Iteration 309, loss = 0.08012071\n",
            "Iteration 310, loss = 0.08013707\n",
            "Iteration 311, loss = 0.07961857\n",
            "Iteration 312, loss = 0.07945782\n",
            "Iteration 313, loss = 0.07949833\n",
            "Iteration 314, loss = 0.07917046\n",
            "Iteration 315, loss = 0.07954304\n",
            "Iteration 316, loss = 0.07931137\n",
            "Iteration 317, loss = 0.07852218\n",
            "Iteration 318, loss = 0.07786199\n",
            "Iteration 319, loss = 0.07800132\n",
            "Iteration 320, loss = 0.07783414\n",
            "Iteration 321, loss = 0.07740705\n",
            "Iteration 322, loss = 0.07748141\n",
            "Iteration 323, loss = 0.07716819\n",
            "Iteration 324, loss = 0.07690205\n",
            "Iteration 325, loss = 0.07748336\n",
            "Iteration 326, loss = 0.07638358\n",
            "Iteration 327, loss = 0.07614030\n",
            "Iteration 328, loss = 0.07595721\n",
            "Iteration 329, loss = 0.07568709\n",
            "Iteration 330, loss = 0.07563815\n",
            "Iteration 331, loss = 0.07525118\n",
            "Iteration 332, loss = 0.07568965\n",
            "Iteration 333, loss = 0.07522880\n",
            "Iteration 334, loss = 0.07477054\n",
            "Iteration 335, loss = 0.07481138\n",
            "Iteration 336, loss = 0.07478170\n",
            "Iteration 337, loss = 0.07401580\n",
            "Iteration 338, loss = 0.07400193\n",
            "Iteration 339, loss = 0.07369849\n",
            "Iteration 340, loss = 0.07347989\n",
            "Iteration 341, loss = 0.07332184\n",
            "Iteration 342, loss = 0.07340831\n",
            "Iteration 343, loss = 0.07368193\n",
            "Iteration 344, loss = 0.07348075\n",
            "Iteration 345, loss = 0.07277238\n",
            "Iteration 346, loss = 0.07225705\n",
            "Iteration 347, loss = 0.07219775\n",
            "Iteration 348, loss = 0.07201925\n",
            "Iteration 349, loss = 0.07195334\n",
            "Iteration 350, loss = 0.07171255\n",
            "Iteration 351, loss = 0.07139893\n",
            "Iteration 352, loss = 0.07138311\n",
            "Iteration 353, loss = 0.07113931\n",
            "Iteration 354, loss = 0.07095232\n",
            "Iteration 355, loss = 0.07079597\n",
            "Iteration 356, loss = 0.07057131\n",
            "Iteration 357, loss = 0.07071015\n",
            "Iteration 358, loss = 0.07011314\n",
            "Iteration 359, loss = 0.07015916\n",
            "Iteration 360, loss = 0.06990186\n",
            "Iteration 361, loss = 0.06969427\n",
            "Iteration 362, loss = 0.06934108\n",
            "Iteration 363, loss = 0.06918674\n",
            "Iteration 364, loss = 0.06902536\n",
            "Iteration 365, loss = 0.06895607\n",
            "Iteration 366, loss = 0.06880855\n",
            "Iteration 367, loss = 0.06855993\n",
            "Iteration 368, loss = 0.06832110\n",
            "Iteration 369, loss = 0.06814590\n",
            "Iteration 370, loss = 0.06799852\n",
            "Iteration 371, loss = 0.06781267\n",
            "Iteration 372, loss = 0.06790416\n",
            "Iteration 373, loss = 0.06750885\n",
            "Iteration 374, loss = 0.06758215\n",
            "Iteration 375, loss = 0.06725171\n",
            "Iteration 376, loss = 0.06741131\n",
            "Iteration 377, loss = 0.06763175\n",
            "Iteration 378, loss = 0.06675139\n",
            "Iteration 379, loss = 0.06655348\n",
            "Iteration 380, loss = 0.06629805\n",
            "Iteration 381, loss = 0.06610107\n",
            "Iteration 382, loss = 0.06629000\n",
            "Iteration 383, loss = 0.06602632\n",
            "Iteration 384, loss = 0.06608394\n",
            "Iteration 385, loss = 0.06580629\n",
            "Iteration 386, loss = 0.06538998\n",
            "Iteration 387, loss = 0.06586715\n",
            "Iteration 388, loss = 0.06515734\n",
            "Iteration 389, loss = 0.06496062\n",
            "Iteration 390, loss = 0.06550206\n",
            "Iteration 391, loss = 0.06457302\n",
            "Iteration 392, loss = 0.06458677\n",
            "Iteration 393, loss = 0.06480695\n",
            "Iteration 394, loss = 0.06436461\n",
            "Iteration 395, loss = 0.06424870\n",
            "Iteration 396, loss = 0.06383528\n",
            "Iteration 397, loss = 0.06417510\n",
            "Iteration 398, loss = 0.06351603\n",
            "Iteration 399, loss = 0.06338986\n",
            "Iteration 400, loss = 0.06335482\n",
            "Iteration 401, loss = 0.06320366\n",
            "Iteration 402, loss = 0.06317608\n",
            "Iteration 403, loss = 0.06291869\n",
            "Iteration 404, loss = 0.06296503\n",
            "Iteration 405, loss = 0.06276617\n",
            "Iteration 406, loss = 0.06240110\n",
            "Iteration 407, loss = 0.06224678\n",
            "Iteration 408, loss = 0.06211562\n",
            "Iteration 409, loss = 0.06223797\n",
            "Iteration 410, loss = 0.06197396\n",
            "Iteration 411, loss = 0.06167804\n",
            "Iteration 412, loss = 0.06246776\n",
            "Iteration 413, loss = 0.06151840\n",
            "Iteration 414, loss = 0.06124658\n",
            "Iteration 415, loss = 0.06126012\n",
            "Iteration 416, loss = 0.06135422\n",
            "Iteration 417, loss = 0.06086002\n",
            "Iteration 418, loss = 0.06069419\n",
            "Iteration 419, loss = 0.06061186\n",
            "Iteration 420, loss = 0.06048001\n",
            "Iteration 421, loss = 0.06074834\n",
            "Iteration 422, loss = 0.06038405\n",
            "Iteration 423, loss = 0.06006450\n",
            "Iteration 424, loss = 0.06001225\n",
            "Iteration 425, loss = 0.05993702\n",
            "Iteration 426, loss = 0.05974584\n",
            "Iteration 427, loss = 0.05965676\n",
            "Iteration 428, loss = 0.05950243\n",
            "Iteration 429, loss = 0.05924844\n",
            "Iteration 430, loss = 0.05911713\n",
            "Iteration 431, loss = 0.05919610\n",
            "Iteration 432, loss = 0.05918844\n",
            "Iteration 433, loss = 0.05931606\n",
            "Iteration 434, loss = 0.05929231\n",
            "Iteration 435, loss = 0.05872098\n",
            "Iteration 436, loss = 0.05850427\n",
            "Iteration 437, loss = 0.05825597\n",
            "Iteration 438, loss = 0.05823798\n",
            "Iteration 439, loss = 0.05812186\n",
            "Iteration 440, loss = 0.05808936\n",
            "Iteration 441, loss = 0.05787239\n",
            "Iteration 442, loss = 0.05769533\n",
            "Iteration 443, loss = 0.05783861\n",
            "Iteration 444, loss = 0.05761503\n",
            "Iteration 445, loss = 0.05739688\n",
            "Iteration 446, loss = 0.05749991\n",
            "Iteration 447, loss = 0.05711061\n",
            "Iteration 448, loss = 0.05712427\n",
            "Iteration 449, loss = 0.05704737\n",
            "Iteration 450, loss = 0.05713448\n",
            "Iteration 451, loss = 0.05673226\n",
            "Iteration 452, loss = 0.05684616\n",
            "Iteration 453, loss = 0.05663205\n",
            "Iteration 454, loss = 0.05671953\n",
            "Iteration 455, loss = 0.05715825\n",
            "Iteration 456, loss = 0.05643812\n",
            "Iteration 457, loss = 0.05589057\n",
            "Iteration 458, loss = 0.05578228\n",
            "Iteration 459, loss = 0.05570304\n",
            "Iteration 460, loss = 0.05567857\n",
            "Iteration 461, loss = 0.05560030\n",
            "Iteration 462, loss = 0.05553276\n",
            "Iteration 463, loss = 0.05560209\n",
            "Iteration 464, loss = 0.05532800\n",
            "Iteration 465, loss = 0.05514093\n",
            "Iteration 466, loss = 0.05519664\n",
            "Iteration 467, loss = 0.05481324\n",
            "Iteration 468, loss = 0.05467839\n",
            "Iteration 469, loss = 0.05542275\n",
            "Iteration 470, loss = 0.05503681\n",
            "Iteration 471, loss = 0.05436250\n",
            "Iteration 472, loss = 0.05418690\n",
            "Iteration 473, loss = 0.05419652\n",
            "Iteration 474, loss = 0.05427280\n",
            "Iteration 475, loss = 0.05410991\n",
            "Iteration 476, loss = 0.05379414\n",
            "Iteration 477, loss = 0.05388757\n",
            "Iteration 478, loss = 0.05384121\n",
            "Iteration 479, loss = 0.05368906\n",
            "Iteration 480, loss = 0.05343046\n",
            "Iteration 481, loss = 0.05343711\n",
            "Iteration 482, loss = 0.05362956\n",
            "Iteration 483, loss = 0.05343542\n",
            "Iteration 484, loss = 0.05332622\n",
            "Iteration 485, loss = 0.05289135\n",
            "Iteration 486, loss = 0.05288413\n",
            "Iteration 487, loss = 0.05288971\n",
            "Iteration 488, loss = 0.05275247\n",
            "Iteration 489, loss = 0.05325079\n",
            "Iteration 490, loss = 0.05354107\n",
            "Iteration 491, loss = 0.05228571\n",
            "Iteration 492, loss = 0.05232484\n",
            "Iteration 493, loss = 0.05219194\n",
            "Iteration 494, loss = 0.05246220\n",
            "Iteration 495, loss = 0.05223247\n",
            "Iteration 496, loss = 0.05184027\n",
            "Iteration 497, loss = 0.05186973\n",
            "Iteration 498, loss = 0.05165202\n",
            "Iteration 499, loss = 0.05157113\n",
            "Iteration 500, loss = 0.05178821\n",
            "Iteration 501, loss = 0.05165321\n",
            "Iteration 502, loss = 0.05146907\n",
            "Iteration 503, loss = 0.05125499\n",
            "Iteration 504, loss = 0.05109740\n",
            "Iteration 505, loss = 0.05099468\n",
            "Iteration 506, loss = 0.05084914\n",
            "Iteration 507, loss = 0.05094579\n",
            "Iteration 508, loss = 0.05093698\n",
            "Iteration 509, loss = 0.05069667\n",
            "Iteration 510, loss = 0.05068961\n",
            "Iteration 511, loss = 0.05069455\n",
            "Iteration 512, loss = 0.05029360\n",
            "Iteration 513, loss = 0.05024604\n",
            "Iteration 514, loss = 0.05069863\n",
            "Iteration 515, loss = 0.05006666\n",
            "Iteration 516, loss = 0.05004312\n",
            "Iteration 517, loss = 0.04980843\n",
            "Iteration 518, loss = 0.04981102\n",
            "Iteration 519, loss = 0.04972312\n",
            "Iteration 520, loss = 0.05017696\n",
            "Iteration 521, loss = 0.04978967\n",
            "Iteration 522, loss = 0.04993126\n",
            "Iteration 523, loss = 0.04932537\n",
            "Iteration 524, loss = 0.04928544\n",
            "Iteration 525, loss = 0.04952735\n",
            "Iteration 526, loss = 0.04958495\n",
            "Iteration 527, loss = 0.04954362\n",
            "Iteration 528, loss = 0.04948942\n",
            "Iteration 529, loss = 0.04892975\n",
            "Iteration 530, loss = 0.04874664\n",
            "Iteration 531, loss = 0.04864559\n",
            "Iteration 532, loss = 0.04858863\n",
            "Iteration 533, loss = 0.04842802\n",
            "Iteration 534, loss = 0.04840140\n",
            "Iteration 535, loss = 0.04826336\n",
            "Iteration 536, loss = 0.04845014\n",
            "Iteration 537, loss = 0.04825009\n",
            "Iteration 538, loss = 0.04805177\n",
            "Iteration 539, loss = 0.04806604\n",
            "Iteration 540, loss = 0.04793806\n",
            "Iteration 541, loss = 0.04768183\n",
            "Iteration 542, loss = 0.04796972\n",
            "Iteration 543, loss = 0.04759982\n",
            "Iteration 544, loss = 0.04773340\n",
            "Iteration 545, loss = 0.04754367\n",
            "Iteration 546, loss = 0.04725663\n",
            "Iteration 547, loss = 0.04738929\n",
            "Iteration 548, loss = 0.04765470\n",
            "Iteration 549, loss = 0.04721210\n",
            "Iteration 550, loss = 0.04707316\n",
            "Iteration 551, loss = 0.04688504\n",
            "Iteration 552, loss = 0.04713105\n",
            "Iteration 553, loss = 0.04692348\n",
            "Iteration 554, loss = 0.04699255\n",
            "Iteration 555, loss = 0.04683715\n",
            "Iteration 556, loss = 0.04670751\n",
            "Iteration 557, loss = 0.04650222\n",
            "Iteration 558, loss = 0.04647618\n",
            "Iteration 559, loss = 0.04622360\n",
            "Iteration 560, loss = 0.04644745\n",
            "Iteration 561, loss = 0.04709419\n",
            "Iteration 562, loss = 0.04629875\n",
            "Iteration 563, loss = 0.04696956\n",
            "Iteration 564, loss = 0.04582839\n",
            "Iteration 565, loss = 0.04617538\n",
            "Iteration 566, loss = 0.04567513\n",
            "Iteration 567, loss = 0.04571662\n",
            "Iteration 568, loss = 0.04560568\n",
            "Iteration 569, loss = 0.04568476\n",
            "Iteration 570, loss = 0.04578780\n",
            "Iteration 571, loss = 0.04567768\n",
            "Iteration 572, loss = 0.04542549\n",
            "Iteration 573, loss = 0.04558145\n",
            "Iteration 574, loss = 0.04527402\n",
            "Iteration 575, loss = 0.04520662\n",
            "Iteration 576, loss = 0.04495003\n",
            "Iteration 577, loss = 0.04481464\n",
            "Iteration 578, loss = 0.04483396\n",
            "Iteration 579, loss = 0.04500700\n",
            "Iteration 580, loss = 0.04522834\n",
            "Iteration 581, loss = 0.04521439\n",
            "Iteration 582, loss = 0.04500054\n",
            "Iteration 583, loss = 0.04545430\n",
            "Iteration 584, loss = 0.04453863\n",
            "Iteration 585, loss = 0.04496152\n",
            "Iteration 586, loss = 0.04454071\n",
            "Iteration 587, loss = 0.04441944\n",
            "Iteration 588, loss = 0.04425172\n",
            "Iteration 589, loss = 0.04402481\n",
            "Iteration 590, loss = 0.04390050\n",
            "Iteration 591, loss = 0.04390813\n",
            "Iteration 592, loss = 0.04372367\n",
            "Iteration 593, loss = 0.04416917\n",
            "Iteration 594, loss = 0.04377977\n",
            "Iteration 595, loss = 0.04366756\n",
            "Iteration 596, loss = 0.04360357\n",
            "Iteration 597, loss = 0.04383634\n",
            "Iteration 598, loss = 0.04353065\n",
            "Iteration 599, loss = 0.04382339\n",
            "Iteration 600, loss = 0.04333232\n",
            "Iteration 601, loss = 0.04330144\n",
            "Iteration 602, loss = 0.04384383\n",
            "Iteration 603, loss = 0.04341088\n",
            "Iteration 604, loss = 0.04330866\n",
            "Iteration 605, loss = 0.04297756\n",
            "Iteration 606, loss = 0.04300315\n",
            "Iteration 607, loss = 0.04298650\n",
            "Iteration 608, loss = 0.04278074\n",
            "Iteration 609, loss = 0.04261629\n",
            "Iteration 610, loss = 0.04286946\n",
            "Iteration 611, loss = 0.04254108\n",
            "Iteration 612, loss = 0.04252249\n",
            "Iteration 613, loss = 0.04303040\n",
            "Iteration 614, loss = 0.04266595\n",
            "Iteration 615, loss = 0.04233914\n",
            "Iteration 616, loss = 0.04210070\n",
            "Iteration 617, loss = 0.04225846\n",
            "Iteration 618, loss = 0.04219245\n",
            "Iteration 619, loss = 0.04212309\n",
            "Iteration 620, loss = 0.04194619\n",
            "Iteration 621, loss = 0.04219635\n",
            "Iteration 622, loss = 0.04199273\n",
            "Iteration 623, loss = 0.04189479\n",
            "Iteration 624, loss = 0.04169307\n",
            "Iteration 625, loss = 0.04202134\n",
            "Iteration 626, loss = 0.04168289\n",
            "Iteration 627, loss = 0.04196922\n",
            "Iteration 628, loss = 0.04164382\n",
            "Iteration 629, loss = 0.04133676\n",
            "Iteration 630, loss = 0.04119151\n",
            "Iteration 631, loss = 0.04136617\n",
            "Iteration 632, loss = 0.04109237\n",
            "Iteration 633, loss = 0.04105880\n",
            "Iteration 634, loss = 0.04107391\n",
            "Iteration 635, loss = 0.04092419\n",
            "Iteration 636, loss = 0.04100710\n",
            "Iteration 637, loss = 0.04090494\n",
            "Iteration 638, loss = 0.04095121\n",
            "Iteration 639, loss = 0.04104179\n",
            "Iteration 640, loss = 0.04084447\n",
            "Iteration 641, loss = 0.04096554\n",
            "Iteration 642, loss = 0.04062258\n",
            "Iteration 643, loss = 0.04048176\n",
            "Iteration 644, loss = 0.04045023\n",
            "Iteration 645, loss = 0.04049956\n",
            "Iteration 646, loss = 0.04056127\n",
            "Iteration 647, loss = 0.04027534\n",
            "Iteration 648, loss = 0.04040526\n",
            "Iteration 649, loss = 0.04026056\n",
            "Iteration 650, loss = 0.04004875\n",
            "Iteration 651, loss = 0.04035627\n",
            "Iteration 652, loss = 0.04058532\n",
            "Iteration 653, loss = 0.03994238\n",
            "Iteration 654, loss = 0.03975422\n",
            "Iteration 655, loss = 0.03976011\n",
            "Iteration 656, loss = 0.03967441\n",
            "Iteration 657, loss = 0.03973965\n",
            "Iteration 658, loss = 0.03952123\n",
            "Iteration 659, loss = 0.03952616\n",
            "Iteration 660, loss = 0.03955049\n",
            "Iteration 661, loss = 0.03937356\n",
            "Iteration 662, loss = 0.03943014\n",
            "Iteration 663, loss = 0.03937631\n",
            "Iteration 664, loss = 0.03929384\n",
            "Iteration 665, loss = 0.03922842\n",
            "Iteration 666, loss = 0.03912863\n",
            "Iteration 667, loss = 0.03944185\n",
            "Iteration 668, loss = 0.03904940\n",
            "Iteration 669, loss = 0.03924999\n",
            "Iteration 670, loss = 0.03908355\n",
            "Iteration 671, loss = 0.03888192\n",
            "Iteration 672, loss = 0.03875379\n",
            "Iteration 673, loss = 0.03882703\n",
            "Iteration 674, loss = 0.03866532\n",
            "Iteration 675, loss = 0.03867650\n",
            "Iteration 676, loss = 0.03861915\n",
            "Iteration 677, loss = 0.03861902\n",
            "Iteration 678, loss = 0.03849354\n",
            "Iteration 679, loss = 0.03853169\n",
            "Iteration 680, loss = 0.03863175\n",
            "Iteration 681, loss = 0.03826541\n",
            "Iteration 682, loss = 0.03867023\n",
            "Iteration 683, loss = 0.03819602\n",
            "Iteration 684, loss = 0.03856383\n",
            "Iteration 685, loss = 0.03879733\n",
            "Iteration 686, loss = 0.03793521\n",
            "Iteration 687, loss = 0.03834029\n",
            "Iteration 688, loss = 0.03810041\n",
            "Iteration 689, loss = 0.03807731\n",
            "Iteration 690, loss = 0.03808654\n",
            "Iteration 691, loss = 0.03775432\n",
            "Iteration 692, loss = 0.03787835\n",
            "Iteration 693, loss = 0.03764072\n",
            "Iteration 694, loss = 0.03762908\n",
            "Iteration 695, loss = 0.03765928\n",
            "Iteration 696, loss = 0.03755746\n",
            "Iteration 697, loss = 0.03744152\n",
            "Iteration 698, loss = 0.03740563\n",
            "Iteration 699, loss = 0.03754415\n",
            "Iteration 700, loss = 0.03794069\n",
            "Iteration 701, loss = 0.03738213\n",
            "Iteration 702, loss = 0.03725410\n",
            "Iteration 703, loss = 0.03704735\n",
            "Iteration 704, loss = 0.03749849\n",
            "Iteration 705, loss = 0.03754051\n",
            "Iteration 706, loss = 0.03778783\n",
            "Iteration 707, loss = 0.03795421\n",
            "Iteration 708, loss = 0.03767247\n",
            "Iteration 709, loss = 0.03667796\n",
            "Iteration 710, loss = 0.03718743\n",
            "Iteration 711, loss = 0.03671721\n",
            "Iteration 712, loss = 0.03676068\n",
            "Iteration 713, loss = 0.03683594\n",
            "Iteration 714, loss = 0.03676476\n",
            "Iteration 715, loss = 0.03682876\n",
            "Iteration 716, loss = 0.03668832\n",
            "Iteration 717, loss = 0.03643443\n",
            "Iteration 718, loss = 0.03673149\n",
            "Iteration 719, loss = 0.03698745\n",
            "Iteration 720, loss = 0.03616054\n",
            "Iteration 721, loss = 0.03651510\n",
            "Iteration 722, loss = 0.03628997\n",
            "Iteration 723, loss = 0.03616607\n",
            "Iteration 724, loss = 0.03608595\n",
            "Iteration 725, loss = 0.03611604\n",
            "Iteration 726, loss = 0.03595784\n",
            "Iteration 727, loss = 0.03597657\n",
            "Iteration 728, loss = 0.03603862\n",
            "Iteration 729, loss = 0.03604859\n",
            "Iteration 730, loss = 0.03581977\n",
            "Iteration 731, loss = 0.03632025\n",
            "Iteration 732, loss = 0.03572935\n",
            "Iteration 733, loss = 0.03566858\n",
            "Iteration 734, loss = 0.03557980\n",
            "Iteration 735, loss = 0.03574934\n",
            "Iteration 736, loss = 0.03563934\n",
            "Iteration 737, loss = 0.03552075\n",
            "Iteration 738, loss = 0.03550817\n",
            "Iteration 739, loss = 0.03532553\n",
            "Iteration 740, loss = 0.03537000\n",
            "Iteration 741, loss = 0.03584305\n",
            "Iteration 742, loss = 0.03533830\n",
            "Iteration 743, loss = 0.03570988\n",
            "Iteration 744, loss = 0.03512450\n",
            "Iteration 745, loss = 0.03528509\n",
            "Iteration 746, loss = 0.03500893\n",
            "Iteration 747, loss = 0.03516964\n",
            "Iteration 748, loss = 0.03498913\n",
            "Iteration 749, loss = 0.03493975\n",
            "Iteration 750, loss = 0.03486854\n",
            "Iteration 751, loss = 0.03489764\n",
            "Iteration 752, loss = 0.03507138\n",
            "Iteration 753, loss = 0.03525178\n",
            "Iteration 754, loss = 0.03474913\n",
            "Iteration 755, loss = 0.03503572\n",
            "Iteration 756, loss = 0.03501975\n",
            "Iteration 757, loss = 0.03453455\n",
            "Iteration 758, loss = 0.03517920\n",
            "Iteration 759, loss = 0.03470176\n",
            "Iteration 760, loss = 0.03435196\n",
            "Iteration 761, loss = 0.03466124\n",
            "Iteration 762, loss = 0.03440671\n",
            "Iteration 763, loss = 0.03468579\n",
            "Iteration 764, loss = 0.03464759\n",
            "Iteration 765, loss = 0.03432357\n",
            "Iteration 766, loss = 0.03426556\n",
            "Iteration 767, loss = 0.03408756\n",
            "Iteration 768, loss = 0.03404899\n",
            "Iteration 769, loss = 0.03410275\n",
            "Iteration 770, loss = 0.03397115\n",
            "Iteration 771, loss = 0.03414707\n",
            "Iteration 772, loss = 0.03393425\n",
            "Iteration 773, loss = 0.03386740\n",
            "Iteration 774, loss = 0.03385401\n",
            "Iteration 775, loss = 0.03391859\n",
            "Iteration 776, loss = 0.03378806\n",
            "Iteration 777, loss = 0.03373200\n",
            "Iteration 778, loss = 0.03368146\n",
            "Iteration 779, loss = 0.03391895\n",
            "Iteration 780, loss = 0.03389737\n",
            "Iteration 781, loss = 0.03358973\n",
            "Iteration 782, loss = 0.03352450\n",
            "Iteration 783, loss = 0.03342438\n",
            "Iteration 784, loss = 0.03373035\n",
            "Iteration 785, loss = 0.03408869\n",
            "Iteration 786, loss = 0.03379172\n",
            "Iteration 787, loss = 0.03327995\n",
            "Iteration 788, loss = 0.03379166\n",
            "Iteration 789, loss = 0.03367210\n",
            "Iteration 790, loss = 0.03332138\n",
            "Iteration 791, loss = 0.03324908\n",
            "Iteration 792, loss = 0.03304078\n",
            "Iteration 793, loss = 0.03321287\n",
            "Iteration 794, loss = 0.03389260\n",
            "Iteration 795, loss = 0.03304592\n",
            "Iteration 796, loss = 0.03306444\n",
            "Iteration 797, loss = 0.03293133\n",
            "Iteration 798, loss = 0.03368491\n",
            "Iteration 799, loss = 0.03337817\n",
            "Iteration 800, loss = 0.03291918\n",
            "Iteration 801, loss = 0.03268275\n",
            "Iteration 802, loss = 0.03314287\n",
            "Iteration 803, loss = 0.03261516\n",
            "Iteration 804, loss = 0.03251959\n",
            "Iteration 805, loss = 0.03290824\n",
            "Iteration 806, loss = 0.03266765\n",
            "Iteration 807, loss = 0.03254912\n",
            "Iteration 808, loss = 0.03272019\n",
            "Iteration 809, loss = 0.03255045\n",
            "Iteration 810, loss = 0.03292790\n",
            "Iteration 811, loss = 0.03274668\n",
            "Iteration 812, loss = 0.03227691\n",
            "Iteration 813, loss = 0.03236504\n",
            "Iteration 814, loss = 0.03226182\n",
            "Iteration 815, loss = 0.03244057\n",
            "Iteration 816, loss = 0.03242982\n",
            "Iteration 817, loss = 0.03255143\n",
            "Iteration 818, loss = 0.03227069\n",
            "Iteration 819, loss = 0.03198898\n",
            "Iteration 820, loss = 0.03252432\n",
            "Iteration 821, loss = 0.03226330\n",
            "Iteration 822, loss = 0.03196371\n",
            "Iteration 823, loss = 0.03184542\n",
            "Iteration 824, loss = 0.03203584\n",
            "Iteration 825, loss = 0.03198924\n",
            "Iteration 826, loss = 0.03181104\n",
            "Iteration 827, loss = 0.03173598\n",
            "Iteration 828, loss = 0.03160239\n",
            "Iteration 829, loss = 0.03162372\n",
            "Iteration 830, loss = 0.03184181\n",
            "Iteration 831, loss = 0.03160299\n",
            "Iteration 832, loss = 0.03149497\n",
            "Iteration 833, loss = 0.03202673\n",
            "Iteration 834, loss = 0.03155915\n",
            "Iteration 835, loss = 0.03200193\n",
            "Iteration 836, loss = 0.03121756\n",
            "Iteration 837, loss = 0.03145957\n",
            "Iteration 838, loss = 0.03154647\n",
            "Iteration 839, loss = 0.03136152\n",
            "Iteration 840, loss = 0.03167245\n",
            "Iteration 841, loss = 0.03183068\n",
            "Iteration 842, loss = 0.03111133\n",
            "Iteration 843, loss = 0.03109798\n",
            "Iteration 844, loss = 0.03118023\n",
            "Iteration 845, loss = 0.03114687\n",
            "Iteration 846, loss = 0.03108412\n",
            "Iteration 847, loss = 0.03110905\n",
            "Iteration 848, loss = 0.03099429\n",
            "Iteration 849, loss = 0.03076200\n",
            "Iteration 850, loss = 0.03092236\n",
            "Iteration 851, loss = 0.03091879\n",
            "Iteration 852, loss = 0.03082526\n",
            "Iteration 853, loss = 0.03111945\n",
            "Iteration 854, loss = 0.03093192\n",
            "Iteration 855, loss = 0.03135797\n",
            "Iteration 856, loss = 0.03081292\n",
            "Iteration 857, loss = 0.03055314\n",
            "Iteration 858, loss = 0.03049917\n",
            "Iteration 859, loss = 0.03055556\n",
            "Iteration 860, loss = 0.03050406\n",
            "Iteration 861, loss = 0.03036672\n",
            "Iteration 862, loss = 0.03050281\n",
            "Iteration 863, loss = 0.03054380\n",
            "Iteration 864, loss = 0.03039646\n",
            "Iteration 865, loss = 0.03036475\n",
            "Iteration 866, loss = 0.03058572\n",
            "Iteration 867, loss = 0.03023315\n",
            "Iteration 868, loss = 0.03025248\n",
            "Iteration 869, loss = 0.03031865\n",
            "Iteration 870, loss = 0.03016584\n",
            "Iteration 871, loss = 0.03007984\n",
            "Iteration 872, loss = 0.03019943\n",
            "Iteration 873, loss = 0.03021801\n",
            "Iteration 874, loss = 0.03018798\n",
            "Iteration 875, loss = 0.03022682\n",
            "Iteration 876, loss = 0.02989211\n",
            "Iteration 877, loss = 0.02990619\n",
            "Iteration 878, loss = 0.02979433\n",
            "Iteration 879, loss = 0.02985512\n",
            "Iteration 880, loss = 0.02978504\n",
            "Iteration 881, loss = 0.02989680\n",
            "Iteration 882, loss = 0.02975535\n",
            "Iteration 883, loss = 0.02964623\n",
            "Iteration 884, loss = 0.02974377\n",
            "Iteration 885, loss = 0.02988967\n",
            "Iteration 886, loss = 0.02969160\n",
            "Iteration 887, loss = 0.02978055\n",
            "Iteration 888, loss = 0.02964421\n",
            "Iteration 889, loss = 0.02981031\n",
            "Iteration 890, loss = 0.02958914\n",
            "Iteration 891, loss = 0.02942349\n",
            "Iteration 892, loss = 0.02967596\n",
            "Iteration 893, loss = 0.02941078\n",
            "Iteration 894, loss = 0.02935451\n",
            "Iteration 895, loss = 0.02932444\n",
            "Iteration 896, loss = 0.02946017\n",
            "Iteration 897, loss = 0.02926136\n",
            "Iteration 898, loss = 0.02943969\n",
            "Iteration 899, loss = 0.02923924\n",
            "Iteration 900, loss = 0.02952226\n",
            "Iteration 901, loss = 0.02922878\n",
            "Iteration 902, loss = 0.02899450\n",
            "Iteration 903, loss = 0.02918850\n",
            "Iteration 904, loss = 0.02912876\n",
            "Iteration 905, loss = 0.02914290\n",
            "Iteration 906, loss = 0.02890760\n",
            "Iteration 907, loss = 0.02903447\n",
            "Iteration 908, loss = 0.02895181\n",
            "Iteration 909, loss = 0.02915549\n",
            "Iteration 910, loss = 0.02889849\n",
            "Iteration 911, loss = 0.02885429\n",
            "Iteration 912, loss = 0.02871349\n",
            "Iteration 913, loss = 0.02873223\n",
            "Iteration 914, loss = 0.02875864\n",
            "Iteration 915, loss = 0.02874733\n",
            "Iteration 916, loss = 0.02865310\n",
            "Iteration 917, loss = 0.02857980\n",
            "Iteration 918, loss = 0.02851721\n",
            "Iteration 919, loss = 0.02881755\n",
            "Iteration 920, loss = 0.02878412\n",
            "Iteration 921, loss = 0.02839030\n",
            "Iteration 922, loss = 0.02855068\n",
            "Iteration 923, loss = 0.02845303\n",
            "Iteration 924, loss = 0.02834575\n",
            "Iteration 925, loss = 0.02833701\n",
            "Iteration 926, loss = 0.02830359\n",
            "Iteration 927, loss = 0.02877540\n",
            "Iteration 928, loss = 0.02829275\n",
            "Iteration 929, loss = 0.02819586\n",
            "Iteration 930, loss = 0.02823714\n",
            "Iteration 931, loss = 0.02812861\n",
            "Iteration 932, loss = 0.02811330\n",
            "Iteration 933, loss = 0.02830829\n",
            "Iteration 934, loss = 0.02815266\n",
            "Iteration 935, loss = 0.02797495\n",
            "Iteration 936, loss = 0.02799643\n",
            "Iteration 937, loss = 0.02797293\n",
            "Iteration 938, loss = 0.02804890\n",
            "Iteration 939, loss = 0.02797220\n",
            "Iteration 940, loss = 0.02794705\n",
            "Iteration 941, loss = 0.02813885\n",
            "Iteration 942, loss = 0.02802337\n",
            "Iteration 943, loss = 0.02786992\n",
            "Iteration 944, loss = 0.02801538\n",
            "Iteration 945, loss = 0.02779616\n",
            "Iteration 946, loss = 0.02821553\n",
            "Iteration 947, loss = 0.02804854\n",
            "Iteration 948, loss = 0.02859181\n",
            "Iteration 949, loss = 0.02849809\n",
            "Iteration 950, loss = 0.02786865\n",
            "Iteration 951, loss = 0.02767652\n",
            "Iteration 952, loss = 0.02784192\n",
            "Iteration 953, loss = 0.02755045\n",
            "Iteration 954, loss = 0.02743275\n",
            "Iteration 955, loss = 0.02760251\n",
            "Iteration 956, loss = 0.02742547\n",
            "Iteration 957, loss = 0.02762334\n",
            "Iteration 958, loss = 0.02735399\n",
            "Iteration 959, loss = 0.02730571\n",
            "Iteration 960, loss = 0.02728803\n",
            "Iteration 961, loss = 0.02721950\n",
            "Iteration 962, loss = 0.02727916\n",
            "Iteration 963, loss = 0.02724468\n",
            "Iteration 964, loss = 0.02752660\n",
            "Iteration 965, loss = 0.02755039\n",
            "Iteration 966, loss = 0.02717923\n",
            "Iteration 967, loss = 0.02704041\n",
            "Iteration 968, loss = 0.02733724\n",
            "Iteration 969, loss = 0.02696823\n",
            "Iteration 970, loss = 0.02694478\n",
            "Iteration 971, loss = 0.02701050\n",
            "Iteration 972, loss = 0.02698236\n",
            "Iteration 973, loss = 0.02722106\n",
            "Iteration 974, loss = 0.02685929\n",
            "Iteration 975, loss = 0.02678421\n",
            "Iteration 976, loss = 0.02701930\n",
            "Iteration 977, loss = 0.02695143\n",
            "Iteration 978, loss = 0.02675594\n",
            "Iteration 979, loss = 0.02671315\n",
            "Iteration 980, loss = 0.02692160\n",
            "Iteration 981, loss = 0.02672248\n",
            "Iteration 982, loss = 0.02683492\n",
            "Iteration 983, loss = 0.02670410\n",
            "Iteration 984, loss = 0.02659826\n",
            "Iteration 985, loss = 0.02668110\n",
            "Iteration 986, loss = 0.02670015\n",
            "Iteration 987, loss = 0.02668708\n",
            "Iteration 988, loss = 0.02651175\n",
            "Iteration 989, loss = 0.02684746\n",
            "Iteration 990, loss = 0.02640266\n",
            "Iteration 991, loss = 0.02656029\n",
            "Iteration 992, loss = 0.02651788\n",
            "Iteration 993, loss = 0.02647134\n",
            "Iteration 994, loss = 0.02636288\n",
            "Iteration 995, loss = 0.02644377\n",
            "Iteration 996, loss = 0.02632531\n",
            "Iteration 997, loss = 0.02656891\n",
            "Iteration 998, loss = 0.02680053\n",
            "Iteration 999, loss = 0.02641435\n",
            "Iteration 1000, loss = 0.02626922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obDDXeI9Qa5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f1bda4-ba52-4308-a293-b1eaef626279"
      },
      "source": [
        "nnModelSGD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(90,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=1, shuffle=True, solver='sgd',\n",
              "              tol=1e-19, validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlKQUu38Qa5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e9f49e-8ee0-4800-e446-46d3be2157ec"
      },
      "source": [
        "nnModelADAM = mlpADAM.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.59517626\n",
            "Iteration 2, loss = 0.92930474\n",
            "Iteration 3, loss = 0.68930840\n",
            "Iteration 4, loss = 0.53547563\n",
            "Iteration 5, loss = 0.44420803\n",
            "Iteration 6, loss = 0.37473163\n",
            "Iteration 7, loss = 0.33372862\n",
            "Iteration 8, loss = 0.28945207\n",
            "Iteration 9, loss = 0.26038227\n",
            "Iteration 10, loss = 0.23424942\n",
            "Iteration 11, loss = 0.21555960\n",
            "Iteration 12, loss = 0.19834890\n",
            "Iteration 13, loss = 0.18399965\n",
            "Iteration 14, loss = 0.16912453\n",
            "Iteration 15, loss = 0.17073973\n",
            "Iteration 16, loss = 0.16022392\n",
            "Iteration 17, loss = 0.14256348\n",
            "Iteration 18, loss = 0.13165571\n",
            "Iteration 19, loss = 0.13175521\n",
            "Iteration 20, loss = 0.12397098\n",
            "Iteration 21, loss = 0.11231918\n",
            "Iteration 22, loss = 0.10908676\n",
            "Iteration 23, loss = 0.10392508\n",
            "Iteration 24, loss = 0.09996151\n",
            "Iteration 25, loss = 0.10265924\n",
            "Iteration 26, loss = 0.09482852\n",
            "Iteration 27, loss = 0.08909072\n",
            "Iteration 28, loss = 0.08461465\n",
            "Iteration 29, loss = 0.08374021\n",
            "Iteration 30, loss = 0.08142495\n",
            "Iteration 31, loss = 0.07719701\n",
            "Iteration 32, loss = 0.08153974\n",
            "Iteration 33, loss = 0.07683893\n",
            "Iteration 34, loss = 0.06953019\n",
            "Iteration 35, loss = 0.06823154\n",
            "Iteration 36, loss = 0.06582522\n",
            "Iteration 37, loss = 0.06342339\n",
            "Iteration 38, loss = 0.06338676\n",
            "Iteration 39, loss = 0.06846483\n",
            "Iteration 40, loss = 0.05985663\n",
            "Iteration 41, loss = 0.05669057\n",
            "Iteration 42, loss = 0.05708963\n",
            "Iteration 43, loss = 0.05438135\n",
            "Iteration 44, loss = 0.05453272\n",
            "Iteration 45, loss = 0.05824497\n",
            "Iteration 46, loss = 0.05120114\n",
            "Iteration 47, loss = 0.04950887\n",
            "Iteration 48, loss = 0.04998845\n",
            "Iteration 49, loss = 0.05080621\n",
            "Iteration 50, loss = 0.04551913\n",
            "Iteration 51, loss = 0.04571145\n",
            "Iteration 52, loss = 0.04488087\n",
            "Iteration 53, loss = 0.04397844\n",
            "Iteration 54, loss = 0.05067411\n",
            "Iteration 55, loss = 0.04701342\n",
            "Iteration 56, loss = 0.04977330\n",
            "Iteration 57, loss = 0.04483733\n",
            "Iteration 58, loss = 0.04223490\n",
            "Iteration 59, loss = 0.03792527\n",
            "Iteration 60, loss = 0.04073835\n",
            "Iteration 61, loss = 0.04434962\n",
            "Iteration 62, loss = 0.03612926\n",
            "Iteration 63, loss = 0.03451143\n",
            "Iteration 64, loss = 0.03451448\n",
            "Iteration 65, loss = 0.03611814\n",
            "Iteration 66, loss = 0.03287606\n",
            "Iteration 67, loss = 0.03350069\n",
            "Iteration 68, loss = 0.03191381\n",
            "Iteration 69, loss = 0.03141030\n",
            "Iteration 70, loss = 0.02975467\n",
            "Iteration 71, loss = 0.03089406\n",
            "Iteration 72, loss = 0.02979334\n",
            "Iteration 73, loss = 0.02963493\n",
            "Iteration 74, loss = 0.03002932\n",
            "Iteration 75, loss = 0.02732479\n",
            "Iteration 76, loss = 0.02706388\n",
            "Iteration 77, loss = 0.02671491\n",
            "Iteration 78, loss = 0.02683492\n",
            "Iteration 79, loss = 0.02667755\n",
            "Iteration 80, loss = 0.03011050\n",
            "Iteration 81, loss = 0.03219709\n",
            "Iteration 82, loss = 0.03085980\n",
            "Iteration 83, loss = 0.02848421\n",
            "Iteration 84, loss = 0.02510176\n",
            "Iteration 85, loss = 0.02293285\n",
            "Iteration 86, loss = 0.02289614\n",
            "Iteration 87, loss = 0.02413946\n",
            "Iteration 88, loss = 0.02254917\n",
            "Iteration 89, loss = 0.02326255\n",
            "Iteration 90, loss = 0.02183848\n",
            "Iteration 91, loss = 0.02565778\n",
            "Iteration 92, loss = 0.02328934\n",
            "Iteration 93, loss = 0.02274646\n",
            "Iteration 94, loss = 0.02172120\n",
            "Iteration 95, loss = 0.01949434\n",
            "Iteration 96, loss = 0.02034016\n",
            "Iteration 97, loss = 0.01831478\n",
            "Iteration 98, loss = 0.02012638\n",
            "Iteration 99, loss = 0.01819852\n",
            "Iteration 100, loss = 0.02444395\n",
            "Iteration 101, loss = 0.02352372\n",
            "Iteration 102, loss = 0.02585160\n",
            "Iteration 103, loss = 0.02659311\n",
            "Iteration 104, loss = 0.02323944\n",
            "Iteration 105, loss = 0.01758200\n",
            "Iteration 106, loss = 0.01658094\n",
            "Iteration 107, loss = 0.01720665\n",
            "Iteration 108, loss = 0.01547734\n",
            "Iteration 109, loss = 0.01501249\n",
            "Iteration 110, loss = 0.01532972\n",
            "Iteration 111, loss = 0.01562086\n",
            "Iteration 112, loss = 0.01837923\n",
            "Iteration 113, loss = 0.01572279\n",
            "Iteration 114, loss = 0.01572312\n",
            "Iteration 115, loss = 0.01336598\n",
            "Iteration 116, loss = 0.01412037\n",
            "Iteration 117, loss = 0.01421950\n",
            "Iteration 118, loss = 0.01417242\n",
            "Iteration 119, loss = 0.01428205\n",
            "Iteration 120, loss = 0.01377240\n",
            "Iteration 121, loss = 0.01337904\n",
            "Iteration 122, loss = 0.01280431\n",
            "Iteration 123, loss = 0.01232698\n",
            "Iteration 124, loss = 0.01147773\n",
            "Iteration 125, loss = 0.01161485\n",
            "Iteration 126, loss = 0.01156595\n",
            "Iteration 127, loss = 0.01166904\n",
            "Iteration 128, loss = 0.01083349\n",
            "Iteration 129, loss = 0.01083082\n",
            "Iteration 130, loss = 0.01065574\n",
            "Iteration 131, loss = 0.01061807\n",
            "Iteration 132, loss = 0.01074234\n",
            "Iteration 133, loss = 0.01083083\n",
            "Iteration 134, loss = 0.01186245\n",
            "Iteration 135, loss = 0.01142002\n",
            "Iteration 136, loss = 0.01093489\n",
            "Iteration 137, loss = 0.01053185\n",
            "Iteration 138, loss = 0.00958287\n",
            "Iteration 139, loss = 0.00939630\n",
            "Iteration 140, loss = 0.00902482\n",
            "Iteration 141, loss = 0.00869861\n",
            "Iteration 142, loss = 0.00981108\n",
            "Iteration 143, loss = 0.01089476\n",
            "Iteration 144, loss = 0.00843222\n",
            "Iteration 145, loss = 0.00828177\n",
            "Iteration 146, loss = 0.00803808\n",
            "Iteration 147, loss = 0.00842048\n",
            "Iteration 148, loss = 0.00815940\n",
            "Iteration 149, loss = 0.00821024\n",
            "Iteration 150, loss = 0.00809590\n",
            "Iteration 151, loss = 0.00842450\n",
            "Iteration 152, loss = 0.00859738\n",
            "Iteration 153, loss = 0.00915160\n",
            "Iteration 154, loss = 0.00821107\n",
            "Iteration 155, loss = 0.00832139\n",
            "Iteration 156, loss = 0.00747195\n",
            "Iteration 157, loss = 0.00706462\n",
            "Iteration 158, loss = 0.00764664\n",
            "Iteration 159, loss = 0.00718727\n",
            "Iteration 160, loss = 0.00989456\n",
            "Iteration 161, loss = 0.01143860\n",
            "Iteration 162, loss = 0.00925103\n",
            "Iteration 163, loss = 0.00684213\n",
            "Iteration 164, loss = 0.00647466\n",
            "Iteration 165, loss = 0.00694124\n",
            "Iteration 166, loss = 0.00771331\n",
            "Iteration 167, loss = 0.00679443\n",
            "Iteration 168, loss = 0.00680496\n",
            "Iteration 169, loss = 0.00622754\n",
            "Iteration 170, loss = 0.00631835\n",
            "Iteration 171, loss = 0.00678747\n",
            "Iteration 172, loss = 0.00616333\n",
            "Iteration 173, loss = 0.00582451\n",
            "Iteration 174, loss = 0.00586249\n",
            "Iteration 175, loss = 0.00579221\n",
            "Iteration 176, loss = 0.00560283\n",
            "Iteration 177, loss = 0.00520400\n",
            "Iteration 178, loss = 0.00540216\n",
            "Iteration 179, loss = 0.00511078\n",
            "Iteration 180, loss = 0.00552761\n",
            "Iteration 181, loss = 0.00520414\n",
            "Iteration 182, loss = 0.00513372\n",
            "Iteration 183, loss = 0.00469657\n",
            "Iteration 184, loss = 0.00512035\n",
            "Iteration 185, loss = 0.00496846\n",
            "Iteration 186, loss = 0.00470398\n",
            "Iteration 187, loss = 0.00468638\n",
            "Iteration 188, loss = 0.00433993\n",
            "Iteration 189, loss = 0.00432356\n",
            "Iteration 190, loss = 0.00434547\n",
            "Iteration 191, loss = 0.00515831\n",
            "Iteration 192, loss = 0.00440697\n",
            "Iteration 193, loss = 0.00425486\n",
            "Iteration 194, loss = 0.00413423\n",
            "Iteration 195, loss = 0.00392837\n",
            "Iteration 196, loss = 0.00417135\n",
            "Iteration 197, loss = 0.00412859\n",
            "Iteration 198, loss = 0.00509179\n",
            "Iteration 199, loss = 0.00392840\n",
            "Iteration 200, loss = 0.00396972\n",
            "Iteration 201, loss = 0.00366881\n",
            "Iteration 202, loss = 0.00363719\n",
            "Iteration 203, loss = 0.00375663\n",
            "Iteration 204, loss = 0.00358829\n",
            "Iteration 205, loss = 0.00356899\n",
            "Iteration 206, loss = 0.00347079\n",
            "Iteration 207, loss = 0.00347541\n",
            "Iteration 208, loss = 0.00336451\n",
            "Iteration 209, loss = 0.00339993\n",
            "Iteration 210, loss = 0.00345604\n",
            "Iteration 211, loss = 0.00327816\n",
            "Iteration 212, loss = 0.00327815\n",
            "Iteration 213, loss = 0.00371450\n",
            "Iteration 214, loss = 0.00400735\n",
            "Iteration 215, loss = 0.00322004\n",
            "Iteration 216, loss = 0.00309947\n",
            "Iteration 217, loss = 0.00310316\n",
            "Iteration 218, loss = 0.00301390\n",
            "Iteration 219, loss = 0.00308055\n",
            "Iteration 220, loss = 0.00295409\n",
            "Iteration 221, loss = 0.00311948\n",
            "Iteration 222, loss = 0.00324436\n",
            "Iteration 223, loss = 0.00302155\n",
            "Iteration 224, loss = 0.00297728\n",
            "Iteration 225, loss = 0.00284334\n",
            "Iteration 226, loss = 0.00273390\n",
            "Iteration 227, loss = 0.00345264\n",
            "Iteration 228, loss = 0.00282257\n",
            "Iteration 229, loss = 0.00267585\n",
            "Iteration 230, loss = 0.00264391\n",
            "Iteration 231, loss = 0.00281705\n",
            "Iteration 232, loss = 0.00289009\n",
            "Iteration 233, loss = 0.00264970\n",
            "Iteration 234, loss = 0.00309658\n",
            "Iteration 235, loss = 0.00332871\n",
            "Iteration 236, loss = 0.00321060\n",
            "Iteration 237, loss = 0.00267276\n",
            "Iteration 238, loss = 0.00241454\n",
            "Iteration 239, loss = 0.00243022\n",
            "Iteration 240, loss = 0.00239596\n",
            "Iteration 241, loss = 0.00229898\n",
            "Iteration 242, loss = 0.00227207\n",
            "Iteration 243, loss = 0.00228124\n",
            "Iteration 244, loss = 0.00228228\n",
            "Iteration 245, loss = 0.00224026\n",
            "Iteration 246, loss = 0.00236667\n",
            "Iteration 247, loss = 0.00220458\n",
            "Iteration 248, loss = 0.00218297\n",
            "Iteration 249, loss = 0.00215840\n",
            "Iteration 250, loss = 0.00209691\n",
            "Iteration 251, loss = 0.00209391\n",
            "Iteration 252, loss = 0.00207939\n",
            "Iteration 253, loss = 0.00204637\n",
            "Iteration 254, loss = 0.00203289\n",
            "Iteration 255, loss = 0.00203966\n",
            "Iteration 256, loss = 0.00197281\n",
            "Iteration 257, loss = 0.00201069\n",
            "Iteration 258, loss = 0.00211449\n",
            "Iteration 259, loss = 0.00193005\n",
            "Iteration 260, loss = 0.00191730\n",
            "Iteration 261, loss = 0.00193848\n",
            "Iteration 262, loss = 0.00186434\n",
            "Iteration 263, loss = 0.00186172\n",
            "Iteration 264, loss = 0.00200612\n",
            "Iteration 265, loss = 0.00189264\n",
            "Iteration 266, loss = 0.00191232\n",
            "Iteration 267, loss = 0.00208732\n",
            "Iteration 268, loss = 0.00198455\n",
            "Iteration 269, loss = 0.00177600\n",
            "Iteration 270, loss = 0.00196402\n",
            "Iteration 271, loss = 0.00188615\n",
            "Iteration 272, loss = 0.00176499\n",
            "Iteration 273, loss = 0.00176516\n",
            "Iteration 274, loss = 0.00177699\n",
            "Iteration 275, loss = 0.00192189\n",
            "Iteration 276, loss = 0.00182650\n",
            "Iteration 277, loss = 0.00162042\n",
            "Iteration 278, loss = 0.00176156\n",
            "Iteration 279, loss = 0.00181037\n",
            "Iteration 280, loss = 0.00169243\n",
            "Iteration 281, loss = 0.00160347\n",
            "Iteration 282, loss = 0.00155826\n",
            "Iteration 283, loss = 0.00154129\n",
            "Iteration 284, loss = 0.00152085\n",
            "Iteration 285, loss = 0.00147757\n",
            "Iteration 286, loss = 0.00146954\n",
            "Iteration 287, loss = 0.00148018\n",
            "Iteration 288, loss = 0.00145850\n",
            "Iteration 289, loss = 0.00151908\n",
            "Iteration 290, loss = 0.00158827\n",
            "Iteration 291, loss = 0.00154648\n",
            "Iteration 292, loss = 0.00193378\n",
            "Iteration 293, loss = 0.00144357\n",
            "Iteration 294, loss = 0.00138015\n",
            "Iteration 295, loss = 0.00137480\n",
            "Iteration 296, loss = 0.00141858\n",
            "Iteration 297, loss = 0.00142645\n",
            "Iteration 298, loss = 0.00144762\n",
            "Iteration 299, loss = 0.00141387\n",
            "Iteration 300, loss = 0.00137780\n",
            "Iteration 301, loss = 0.00130993\n",
            "Iteration 302, loss = 0.00131366\n",
            "Iteration 303, loss = 0.00133543\n",
            "Iteration 304, loss = 0.00128915\n",
            "Iteration 305, loss = 0.00126688\n",
            "Iteration 306, loss = 0.00122772\n",
            "Iteration 307, loss = 0.00134196\n",
            "Iteration 308, loss = 0.00121632\n",
            "Iteration 309, loss = 0.00120180\n",
            "Iteration 310, loss = 0.00119153\n",
            "Iteration 311, loss = 0.00119903\n",
            "Iteration 312, loss = 0.00117238\n",
            "Iteration 313, loss = 0.00116440\n",
            "Iteration 314, loss = 0.00115179\n",
            "Iteration 315, loss = 0.00118509\n",
            "Iteration 316, loss = 0.00127366\n",
            "Iteration 317, loss = 0.00111802\n",
            "Iteration 318, loss = 0.00113027\n",
            "Iteration 319, loss = 0.00117635\n",
            "Iteration 320, loss = 0.00111237\n",
            "Iteration 321, loss = 0.00110117\n",
            "Iteration 322, loss = 0.00111806\n",
            "Iteration 323, loss = 0.00114183\n",
            "Iteration 324, loss = 0.00108484\n",
            "Iteration 325, loss = 0.00108386\n",
            "Iteration 326, loss = 0.00105907\n",
            "Iteration 327, loss = 0.00106360\n",
            "Iteration 328, loss = 0.00103444\n",
            "Iteration 329, loss = 0.00101843\n",
            "Iteration 330, loss = 0.00101012\n",
            "Iteration 331, loss = 0.00101014\n",
            "Iteration 332, loss = 0.00101246\n",
            "Iteration 333, loss = 0.00100957\n",
            "Iteration 334, loss = 0.00106120\n",
            "Iteration 335, loss = 0.00100468\n",
            "Iteration 336, loss = 0.00109222\n",
            "Iteration 337, loss = 0.00101409\n",
            "Iteration 338, loss = 0.00107677\n",
            "Iteration 339, loss = 0.00099358\n",
            "Iteration 340, loss = 0.00098439\n",
            "Iteration 341, loss = 0.00101586\n",
            "Iteration 342, loss = 0.00097389\n",
            "Iteration 343, loss = 0.00096190\n",
            "Iteration 344, loss = 0.00099944\n",
            "Iteration 345, loss = 0.00099203\n",
            "Iteration 346, loss = 0.00108031\n",
            "Iteration 347, loss = 0.00097030\n",
            "Iteration 348, loss = 0.00093268\n",
            "Iteration 349, loss = 0.00090412\n",
            "Iteration 350, loss = 0.00088845\n",
            "Iteration 351, loss = 0.00085971\n",
            "Iteration 352, loss = 0.00085430\n",
            "Iteration 353, loss = 0.00085062\n",
            "Iteration 354, loss = 0.00086026\n",
            "Iteration 355, loss = 0.00086549\n",
            "Iteration 356, loss = 0.00083856\n",
            "Iteration 357, loss = 0.00089463\n",
            "Iteration 358, loss = 0.00090504\n",
            "Iteration 359, loss = 0.00085379\n",
            "Iteration 360, loss = 0.00083821\n",
            "Iteration 361, loss = 0.00081259\n",
            "Iteration 362, loss = 0.00080888\n",
            "Iteration 363, loss = 0.00080102\n",
            "Iteration 364, loss = 0.00078887\n",
            "Iteration 365, loss = 0.00079318\n",
            "Iteration 366, loss = 0.00079533\n",
            "Iteration 367, loss = 0.00077900\n",
            "Iteration 368, loss = 0.00078817\n",
            "Iteration 369, loss = 0.00076396\n",
            "Iteration 370, loss = 0.00077429\n",
            "Iteration 371, loss = 0.00075352\n",
            "Iteration 372, loss = 0.00075018\n",
            "Iteration 373, loss = 0.00077518\n",
            "Iteration 374, loss = 0.00074361\n",
            "Iteration 375, loss = 0.00074847\n",
            "Iteration 376, loss = 0.00077630\n",
            "Iteration 377, loss = 0.00074306\n",
            "Iteration 378, loss = 0.00071990\n",
            "Iteration 379, loss = 0.00071512\n",
            "Iteration 380, loss = 0.00070768\n",
            "Iteration 381, loss = 0.00070411\n",
            "Iteration 382, loss = 0.00069860\n",
            "Iteration 383, loss = 0.00069571\n",
            "Iteration 384, loss = 0.00070467\n",
            "Iteration 385, loss = 0.00071862\n",
            "Iteration 386, loss = 0.00073041\n",
            "Iteration 387, loss = 0.00072001\n",
            "Iteration 388, loss = 0.00068377\n",
            "Iteration 389, loss = 0.00067612\n",
            "Iteration 390, loss = 0.00067912\n",
            "Iteration 391, loss = 0.00067342\n",
            "Iteration 392, loss = 0.00065239\n",
            "Iteration 393, loss = 0.00067923\n",
            "Iteration 394, loss = 0.00065896\n",
            "Iteration 395, loss = 0.00064495\n",
            "Iteration 396, loss = 0.00063424\n",
            "Iteration 397, loss = 0.00064584\n",
            "Iteration 398, loss = 0.00064347\n",
            "Iteration 399, loss = 0.00063152\n",
            "Iteration 400, loss = 0.00062588\n",
            "Iteration 401, loss = 0.00062501\n",
            "Iteration 402, loss = 0.00062193\n",
            "Iteration 403, loss = 0.00062978\n",
            "Iteration 404, loss = 0.00061668\n",
            "Iteration 405, loss = 0.00060824\n",
            "Iteration 406, loss = 0.00059968\n",
            "Iteration 407, loss = 0.00059451\n",
            "Iteration 408, loss = 0.00059247\n",
            "Iteration 409, loss = 0.00059302\n",
            "Iteration 410, loss = 0.00059032\n",
            "Iteration 411, loss = 0.00059461\n",
            "Iteration 412, loss = 0.00060841\n",
            "Iteration 413, loss = 0.00059017\n",
            "Iteration 414, loss = 0.00060920\n",
            "Iteration 415, loss = 0.00058040\n",
            "Iteration 416, loss = 0.00060170\n",
            "Iteration 417, loss = 0.00057461\n",
            "Iteration 418, loss = 0.00057363\n",
            "Iteration 419, loss = 0.00056454\n",
            "Iteration 420, loss = 0.00055806\n",
            "Iteration 421, loss = 0.00056410\n",
            "Iteration 422, loss = 0.00056789\n",
            "Iteration 423, loss = 0.00055927\n",
            "Iteration 424, loss = 0.00053832\n",
            "Iteration 425, loss = 0.00054014\n",
            "Iteration 426, loss = 0.00053266\n",
            "Iteration 427, loss = 0.00053046\n",
            "Iteration 428, loss = 0.00052933\n",
            "Iteration 429, loss = 0.00052764\n",
            "Iteration 430, loss = 0.00052036\n",
            "Iteration 431, loss = 0.00052463\n",
            "Iteration 432, loss = 0.00053257\n",
            "Iteration 433, loss = 0.00053431\n",
            "Iteration 434, loss = 0.00054144\n",
            "Iteration 435, loss = 0.00052501\n",
            "Iteration 436, loss = 0.00051423\n",
            "Iteration 437, loss = 0.00050601\n",
            "Iteration 438, loss = 0.00050232\n",
            "Iteration 439, loss = 0.00050263\n",
            "Iteration 440, loss = 0.00050203\n",
            "Iteration 441, loss = 0.00049763\n",
            "Iteration 442, loss = 0.00049068\n",
            "Iteration 443, loss = 0.00048820\n",
            "Iteration 444, loss = 0.00048693\n",
            "Iteration 445, loss = 0.00048606\n",
            "Iteration 446, loss = 0.00048515\n",
            "Iteration 447, loss = 0.00047790\n",
            "Iteration 448, loss = 0.00048425\n",
            "Iteration 449, loss = 0.00048477\n",
            "Iteration 450, loss = 0.00049322\n",
            "Iteration 451, loss = 0.00048150\n",
            "Iteration 452, loss = 0.00046913\n",
            "Iteration 453, loss = 0.00047281\n",
            "Iteration 454, loss = 0.00047027\n",
            "Iteration 455, loss = 0.00050688\n",
            "Iteration 456, loss = 0.00048650\n",
            "Iteration 457, loss = 0.00046429\n",
            "Iteration 458, loss = 0.00045313\n",
            "Iteration 459, loss = 0.00044697\n",
            "Iteration 460, loss = 0.00044573\n",
            "Iteration 461, loss = 0.00044237\n",
            "Iteration 462, loss = 0.00043818\n",
            "Iteration 463, loss = 0.00044412\n",
            "Iteration 464, loss = 0.00043642\n",
            "Iteration 465, loss = 0.00043032\n",
            "Iteration 466, loss = 0.00043620\n",
            "Iteration 467, loss = 0.00042898\n",
            "Iteration 468, loss = 0.00043049\n",
            "Iteration 469, loss = 0.00042760\n",
            "Iteration 470, loss = 0.00042574\n",
            "Iteration 471, loss = 0.00041860\n",
            "Iteration 472, loss = 0.00041922\n",
            "Iteration 473, loss = 0.00041551\n",
            "Iteration 474, loss = 0.00041565\n",
            "Iteration 475, loss = 0.00041323\n",
            "Iteration 476, loss = 0.00041098\n",
            "Iteration 477, loss = 0.00040941\n",
            "Iteration 478, loss = 0.00040478\n",
            "Iteration 479, loss = 0.00040667\n",
            "Iteration 480, loss = 0.00040318\n",
            "Iteration 481, loss = 0.00040003\n",
            "Iteration 482, loss = 0.00039849\n",
            "Iteration 483, loss = 0.00040152\n",
            "Iteration 484, loss = 0.00039844\n",
            "Iteration 485, loss = 0.00039654\n",
            "Iteration 486, loss = 0.00039181\n",
            "Iteration 487, loss = 0.00039097\n",
            "Iteration 488, loss = 0.00038779\n",
            "Iteration 489, loss = 0.00039288\n",
            "Iteration 490, loss = 0.00041532\n",
            "Iteration 491, loss = 0.00040279\n",
            "Iteration 492, loss = 0.00039554\n",
            "Iteration 493, loss = 0.00038587\n",
            "Iteration 494, loss = 0.00038200\n",
            "Iteration 495, loss = 0.00037629\n",
            "Iteration 496, loss = 0.00037496\n",
            "Iteration 497, loss = 0.00037676\n",
            "Iteration 498, loss = 0.00037497\n",
            "Iteration 499, loss = 0.00036986\n",
            "Iteration 500, loss = 0.00037079\n",
            "Iteration 501, loss = 0.00036878\n",
            "Iteration 502, loss = 0.00036842\n",
            "Iteration 503, loss = 0.00036837\n",
            "Iteration 504, loss = 0.00036496\n",
            "Iteration 505, loss = 0.00035810\n",
            "Iteration 506, loss = 0.00035576\n",
            "Iteration 507, loss = 0.00035501\n",
            "Iteration 508, loss = 0.00035192\n",
            "Iteration 509, loss = 0.00034734\n",
            "Iteration 510, loss = 0.00034630\n",
            "Iteration 511, loss = 0.00035023\n",
            "Iteration 512, loss = 0.00034176\n",
            "Iteration 513, loss = 0.00034367\n",
            "Iteration 514, loss = 0.00035885\n",
            "Iteration 515, loss = 0.00033616\n",
            "Iteration 516, loss = 0.00034537\n",
            "Iteration 517, loss = 0.00034257\n",
            "Iteration 518, loss = 0.00032988\n",
            "Iteration 519, loss = 0.00032738\n",
            "Iteration 520, loss = 0.00032533\n",
            "Iteration 521, loss = 0.00032046\n",
            "Iteration 522, loss = 0.00032379\n",
            "Iteration 523, loss = 0.00031339\n",
            "Iteration 524, loss = 0.00031168\n",
            "Iteration 525, loss = 0.00032064\n",
            "Iteration 526, loss = 0.00032273\n",
            "Iteration 527, loss = 0.00031605\n",
            "Iteration 528, loss = 0.00032536\n",
            "Iteration 529, loss = 0.00033355\n",
            "Iteration 530, loss = 0.00033128\n",
            "Iteration 531, loss = 0.00031977\n",
            "Iteration 532, loss = 0.00031115\n",
            "Iteration 533, loss = 0.00031229\n",
            "Iteration 534, loss = 0.00029873\n",
            "Iteration 535, loss = 0.00029674\n",
            "Iteration 536, loss = 0.00030139\n",
            "Iteration 537, loss = 0.00029705\n",
            "Iteration 538, loss = 0.00029703\n",
            "Iteration 539, loss = 0.00029634\n",
            "Iteration 540, loss = 0.00028932\n",
            "Iteration 541, loss = 0.00029080\n",
            "Iteration 542, loss = 0.00029600\n",
            "Iteration 543, loss = 0.00029087\n",
            "Iteration 544, loss = 0.00029122\n",
            "Iteration 545, loss = 0.00029293\n",
            "Iteration 546, loss = 0.00028340\n",
            "Iteration 547, loss = 0.00028263\n",
            "Iteration 548, loss = 0.00028027\n",
            "Iteration 549, loss = 0.00028199\n",
            "Iteration 550, loss = 0.00027860\n",
            "Iteration 551, loss = 0.00028071\n",
            "Iteration 552, loss = 0.00028229\n",
            "Iteration 553, loss = 0.00027574\n",
            "Iteration 554, loss = 0.00027678\n",
            "Iteration 555, loss = 0.00027456\n",
            "Iteration 556, loss = 0.00027328\n",
            "Iteration 557, loss = 0.00027163\n",
            "Iteration 558, loss = 0.00027177\n",
            "Iteration 559, loss = 0.00026924\n",
            "Iteration 560, loss = 0.00027032\n",
            "Iteration 561, loss = 0.00027122\n",
            "Iteration 562, loss = 0.00026664\n",
            "Iteration 563, loss = 0.00027513\n",
            "Iteration 564, loss = 0.00027091\n",
            "Iteration 565, loss = 0.00026577\n",
            "Iteration 566, loss = 0.00026572\n",
            "Iteration 567, loss = 0.00026174\n",
            "Iteration 568, loss = 0.00026230\n",
            "Iteration 569, loss = 0.00026058\n",
            "Iteration 570, loss = 0.00026007\n",
            "Iteration 571, loss = 0.00026043\n",
            "Iteration 572, loss = 0.00026063\n",
            "Iteration 573, loss = 0.00025604\n",
            "Iteration 574, loss = 0.00025553\n",
            "Iteration 575, loss = 0.00025580\n",
            "Iteration 576, loss = 0.00025273\n",
            "Iteration 577, loss = 0.00025503\n",
            "Iteration 578, loss = 0.00025730\n",
            "Iteration 579, loss = 0.00025370\n",
            "Iteration 580, loss = 0.00025786\n",
            "Iteration 581, loss = 0.00025957\n",
            "Iteration 582, loss = 0.00025940\n",
            "Iteration 583, loss = 0.00026028\n",
            "Iteration 584, loss = 0.00025015\n",
            "Iteration 585, loss = 0.00025563\n",
            "Iteration 586, loss = 0.00025350\n",
            "Iteration 587, loss = 0.00025313\n",
            "Iteration 588, loss = 0.00025367\n",
            "Iteration 589, loss = 0.00024527\n",
            "Iteration 590, loss = 0.00024292\n",
            "Iteration 591, loss = 0.00024177\n",
            "Iteration 592, loss = 0.00023962\n",
            "Iteration 593, loss = 0.00024145\n",
            "Iteration 594, loss = 0.00024041\n",
            "Iteration 595, loss = 0.00023699\n",
            "Iteration 596, loss = 0.00023741\n",
            "Iteration 597, loss = 0.00023979\n",
            "Iteration 598, loss = 0.00023640\n",
            "Iteration 599, loss = 0.00023740\n",
            "Iteration 600, loss = 0.00023403\n",
            "Iteration 601, loss = 0.00023226\n",
            "Iteration 602, loss = 0.00023695\n",
            "Iteration 603, loss = 0.00023426\n",
            "Iteration 604, loss = 0.00023317\n",
            "Iteration 605, loss = 0.00023018\n",
            "Iteration 606, loss = 0.00022851\n",
            "Iteration 607, loss = 0.00023009\n",
            "Iteration 608, loss = 0.00022959\n",
            "Iteration 609, loss = 0.00023039\n",
            "Iteration 610, loss = 0.00022674\n",
            "Iteration 611, loss = 0.00023036\n",
            "Iteration 612, loss = 0.00023198\n",
            "Iteration 613, loss = 0.00023488\n",
            "Iteration 614, loss = 0.00022826\n",
            "Iteration 615, loss = 0.00022513\n",
            "Iteration 616, loss = 0.00022241\n",
            "Iteration 617, loss = 0.00022698\n",
            "Iteration 618, loss = 0.00022213\n",
            "Iteration 619, loss = 0.00022094\n",
            "Iteration 620, loss = 0.00021985\n",
            "Iteration 621, loss = 0.00021864\n",
            "Iteration 622, loss = 0.00021895\n",
            "Iteration 623, loss = 0.00021770\n",
            "Iteration 624, loss = 0.00021759\n",
            "Iteration 625, loss = 0.00022070\n",
            "Iteration 626, loss = 0.00021641\n",
            "Iteration 627, loss = 0.00021693\n",
            "Iteration 628, loss = 0.00021872\n",
            "Iteration 629, loss = 0.00021534\n",
            "Iteration 630, loss = 0.00021404\n",
            "Iteration 631, loss = 0.00021228\n",
            "Iteration 632, loss = 0.00021185\n",
            "Iteration 633, loss = 0.00021102\n",
            "Iteration 634, loss = 0.00021043\n",
            "Iteration 635, loss = 0.00020995\n",
            "Iteration 636, loss = 0.00020996\n",
            "Iteration 637, loss = 0.00021007\n",
            "Iteration 638, loss = 0.00020905\n",
            "Iteration 639, loss = 0.00020780\n",
            "Iteration 640, loss = 0.00021004\n",
            "Iteration 641, loss = 0.00021141\n",
            "Iteration 642, loss = 0.00020892\n",
            "Iteration 643, loss = 0.00020617\n",
            "Iteration 644, loss = 0.00020822\n",
            "Iteration 645, loss = 0.00020659\n",
            "Iteration 646, loss = 0.00020508\n",
            "Iteration 647, loss = 0.00020431\n",
            "Iteration 648, loss = 0.00020438\n",
            "Iteration 649, loss = 0.00020405\n",
            "Iteration 650, loss = 0.00020294\n",
            "Iteration 651, loss = 0.00020255\n",
            "Iteration 652, loss = 0.00020265\n",
            "Iteration 653, loss = 0.00020040\n",
            "Iteration 654, loss = 0.00020069\n",
            "Iteration 655, loss = 0.00019979\n",
            "Iteration 656, loss = 0.00019901\n",
            "Iteration 657, loss = 0.00019961\n",
            "Iteration 658, loss = 0.00019727\n",
            "Iteration 659, loss = 0.00019701\n",
            "Iteration 660, loss = 0.00019645\n",
            "Iteration 661, loss = 0.00019668\n",
            "Iteration 662, loss = 0.00019630\n",
            "Iteration 663, loss = 0.00019543\n",
            "Iteration 664, loss = 0.00019580\n",
            "Iteration 665, loss = 0.00019501\n",
            "Iteration 666, loss = 0.00019444\n",
            "Iteration 667, loss = 0.00019524\n",
            "Iteration 668, loss = 0.00019277\n",
            "Iteration 669, loss = 0.00019397\n",
            "Iteration 670, loss = 0.00019393\n",
            "Iteration 671, loss = 0.00019141\n",
            "Iteration 672, loss = 0.00019135\n",
            "Iteration 673, loss = 0.00019192\n",
            "Iteration 674, loss = 0.00019040\n",
            "Iteration 675, loss = 0.00018969\n",
            "Iteration 676, loss = 0.00019081\n",
            "Iteration 677, loss = 0.00019074\n",
            "Iteration 678, loss = 0.00018983\n",
            "Iteration 679, loss = 0.00018865\n",
            "Iteration 680, loss = 0.00018781\n",
            "Iteration 681, loss = 0.00018715\n",
            "Iteration 682, loss = 0.00018761\n",
            "Iteration 683, loss = 0.00018705\n",
            "Iteration 684, loss = 0.00019440\n",
            "Iteration 685, loss = 0.00019687\n",
            "Iteration 686, loss = 0.00019349\n",
            "Iteration 687, loss = 0.00019039\n",
            "Iteration 688, loss = 0.00018457\n",
            "Iteration 689, loss = 0.00018410\n",
            "Iteration 690, loss = 0.00018367\n",
            "Iteration 691, loss = 0.00018333\n",
            "Iteration 692, loss = 0.00018276\n",
            "Iteration 693, loss = 0.00018228\n",
            "Iteration 694, loss = 0.00018196\n",
            "Iteration 695, loss = 0.00018101\n",
            "Iteration 696, loss = 0.00018070\n",
            "Iteration 697, loss = 0.00018139\n",
            "Iteration 698, loss = 0.00018005\n",
            "Iteration 699, loss = 0.00018113\n",
            "Iteration 700, loss = 0.00018461\n",
            "Iteration 701, loss = 0.00018158\n",
            "Iteration 702, loss = 0.00017985\n",
            "Iteration 703, loss = 0.00017748\n",
            "Iteration 704, loss = 0.00017848\n",
            "Iteration 705, loss = 0.00017964\n",
            "Iteration 706, loss = 0.00018123\n",
            "Iteration 707, loss = 0.00018204\n",
            "Iteration 708, loss = 0.00018296\n",
            "Iteration 709, loss = 0.00017686\n",
            "Iteration 710, loss = 0.00017832\n",
            "Iteration 711, loss = 0.00017590\n",
            "Iteration 712, loss = 0.00017591\n",
            "Iteration 713, loss = 0.00017626\n",
            "Iteration 714, loss = 0.00017642\n",
            "Iteration 715, loss = 0.00017652\n",
            "Iteration 716, loss = 0.00017424\n",
            "Iteration 717, loss = 0.00017374\n",
            "Iteration 718, loss = 0.00017555\n",
            "Iteration 719, loss = 0.00017953\n",
            "Iteration 720, loss = 0.00017581\n",
            "Iteration 721, loss = 0.00017277\n",
            "Iteration 722, loss = 0.00017157\n",
            "Iteration 723, loss = 0.00017129\n",
            "Iteration 724, loss = 0.00017141\n",
            "Iteration 725, loss = 0.00017217\n",
            "Iteration 726, loss = 0.00017212\n",
            "Iteration 727, loss = 0.00016941\n",
            "Iteration 728, loss = 0.00017065\n",
            "Iteration 729, loss = 0.00016889\n",
            "Iteration 730, loss = 0.00016863\n",
            "Iteration 731, loss = 0.00017041\n",
            "Iteration 732, loss = 0.00016827\n",
            "Iteration 733, loss = 0.00016800\n",
            "Iteration 734, loss = 0.00016732\n",
            "Iteration 735, loss = 0.00016745\n",
            "Iteration 736, loss = 0.00016707\n",
            "Iteration 737, loss = 0.00016643\n",
            "Iteration 738, loss = 0.00016644\n",
            "Iteration 739, loss = 0.00016575\n",
            "Iteration 740, loss = 0.00016546\n",
            "Iteration 741, loss = 0.00016687\n",
            "Iteration 742, loss = 0.00016540\n",
            "Iteration 743, loss = 0.00016579\n",
            "Iteration 744, loss = 0.00016516\n",
            "Iteration 745, loss = 0.00016549\n",
            "Iteration 746, loss = 0.00016455\n",
            "Iteration 747, loss = 0.00016418\n",
            "Iteration 748, loss = 0.00016424\n",
            "Iteration 749, loss = 0.00016292\n",
            "Iteration 750, loss = 0.00016273\n",
            "Iteration 751, loss = 0.00016213\n",
            "Iteration 752, loss = 0.00016286\n",
            "Iteration 753, loss = 0.00016329\n",
            "Iteration 754, loss = 0.00016206\n",
            "Iteration 755, loss = 0.00016291\n",
            "Iteration 756, loss = 0.00016366\n",
            "Iteration 757, loss = 0.00016194\n",
            "Iteration 758, loss = 0.00016362\n",
            "Iteration 759, loss = 0.00016232\n",
            "Iteration 760, loss = 0.00016079\n",
            "Iteration 761, loss = 0.00015956\n",
            "Iteration 762, loss = 0.00015911\n",
            "Iteration 763, loss = 0.00016067\n",
            "Iteration 764, loss = 0.00016135\n",
            "Iteration 765, loss = 0.00015963\n",
            "Iteration 766, loss = 0.00015853\n",
            "Iteration 767, loss = 0.00015818\n",
            "Iteration 768, loss = 0.00015801\n",
            "Iteration 769, loss = 0.00015789\n",
            "Iteration 770, loss = 0.00015759\n",
            "Iteration 771, loss = 0.00015761\n",
            "Iteration 772, loss = 0.00015687\n",
            "Iteration 773, loss = 0.00015659\n",
            "Iteration 774, loss = 0.00015603\n",
            "Iteration 775, loss = 0.00015598\n",
            "Iteration 776, loss = 0.00015654\n",
            "Iteration 777, loss = 0.00015583\n",
            "Iteration 778, loss = 0.00015568\n",
            "Iteration 779, loss = 0.00015562\n",
            "Iteration 780, loss = 0.00015608\n",
            "Iteration 781, loss = 0.00015447\n",
            "Iteration 782, loss = 0.00015418\n",
            "Iteration 783, loss = 0.00015521\n",
            "Iteration 784, loss = 0.00015432\n",
            "Iteration 785, loss = 0.00015455\n",
            "Iteration 786, loss = 0.00015509\n",
            "Iteration 787, loss = 0.00015359\n",
            "Iteration 788, loss = 0.00015492\n",
            "Iteration 789, loss = 0.00015549\n",
            "Iteration 790, loss = 0.00015405\n",
            "Iteration 791, loss = 0.00015276\n",
            "Iteration 792, loss = 0.00015279\n",
            "Iteration 793, loss = 0.00015260\n",
            "Iteration 794, loss = 0.00015572\n",
            "Iteration 795, loss = 0.00015408\n",
            "Iteration 796, loss = 0.00015155\n",
            "Iteration 797, loss = 0.00015106\n",
            "Iteration 798, loss = 0.00015291\n",
            "Iteration 799, loss = 0.00015406\n",
            "Iteration 800, loss = 0.00015022\n",
            "Iteration 801, loss = 0.00014962\n",
            "Iteration 802, loss = 0.00015165\n",
            "Iteration 803, loss = 0.00014957\n",
            "Iteration 804, loss = 0.00014934\n",
            "Iteration 805, loss = 0.00015027\n",
            "Iteration 806, loss = 0.00014980\n",
            "Iteration 807, loss = 0.00014884\n",
            "Iteration 808, loss = 0.00014874\n",
            "Iteration 809, loss = 0.00014844\n",
            "Iteration 810, loss = 0.00014987\n",
            "Iteration 811, loss = 0.00015049\n",
            "Iteration 812, loss = 0.00014894\n",
            "Iteration 813, loss = 0.00014827\n",
            "Iteration 814, loss = 0.00014809\n",
            "Iteration 815, loss = 0.00014841\n",
            "Iteration 816, loss = 0.00014925\n",
            "Iteration 817, loss = 0.00014973\n",
            "Iteration 818, loss = 0.00014848\n",
            "Iteration 819, loss = 0.00014636\n",
            "Iteration 820, loss = 0.00014720\n",
            "Iteration 821, loss = 0.00014627\n",
            "Iteration 822, loss = 0.00014641\n",
            "Iteration 823, loss = 0.00014556\n",
            "Iteration 824, loss = 0.00014583\n",
            "Iteration 825, loss = 0.00014612\n",
            "Iteration 826, loss = 0.00014606\n",
            "Iteration 827, loss = 0.00014601\n",
            "Iteration 828, loss = 0.00014479\n",
            "Iteration 829, loss = 0.00014435\n",
            "Iteration 830, loss = 0.00014480\n",
            "Iteration 831, loss = 0.00014413\n",
            "Iteration 832, loss = 0.00014428\n",
            "Iteration 833, loss = 0.00014420\n",
            "Iteration 834, loss = 0.00014385\n",
            "Iteration 835, loss = 0.00014481\n",
            "Iteration 836, loss = 0.00014422\n",
            "Iteration 837, loss = 0.00014323\n",
            "Iteration 838, loss = 0.00014425\n",
            "Iteration 839, loss = 0.00014428\n",
            "Iteration 840, loss = 0.00014430\n",
            "Iteration 841, loss = 0.00014360\n",
            "Iteration 842, loss = 0.00014343\n",
            "Iteration 843, loss = 0.00014197\n",
            "Iteration 844, loss = 0.00014181\n",
            "Iteration 845, loss = 0.00014209\n",
            "Iteration 846, loss = 0.00014097\n",
            "Iteration 847, loss = 0.00014127\n",
            "Iteration 848, loss = 0.00014138\n",
            "Iteration 849, loss = 0.00014070\n",
            "Iteration 850, loss = 0.00014069\n",
            "Iteration 851, loss = 0.00014130\n",
            "Iteration 852, loss = 0.00014044\n",
            "Iteration 853, loss = 0.00014079\n",
            "Iteration 854, loss = 0.00014085\n",
            "Iteration 855, loss = 0.00014165\n",
            "Iteration 856, loss = 0.00014178\n",
            "Iteration 857, loss = 0.00014091\n",
            "Iteration 858, loss = 0.00013961\n",
            "Iteration 859, loss = 0.00013925\n",
            "Iteration 860, loss = 0.00013911\n",
            "Iteration 861, loss = 0.00013894\n",
            "Iteration 862, loss = 0.00013876\n",
            "Iteration 863, loss = 0.00013897\n",
            "Iteration 864, loss = 0.00013893\n",
            "Iteration 865, loss = 0.00013858\n",
            "Iteration 866, loss = 0.00013818\n",
            "Iteration 867, loss = 0.00013818\n",
            "Iteration 868, loss = 0.00013845\n",
            "Iteration 869, loss = 0.00013846\n",
            "Iteration 870, loss = 0.00013826\n",
            "Iteration 871, loss = 0.00013764\n",
            "Iteration 872, loss = 0.00013764\n",
            "Iteration 873, loss = 0.00013724\n",
            "Iteration 874, loss = 0.00013733\n",
            "Iteration 875, loss = 0.00013742\n",
            "Iteration 876, loss = 0.00013790\n",
            "Iteration 877, loss = 0.00013899\n",
            "Iteration 878, loss = 0.00013733\n",
            "Iteration 879, loss = 0.00013763\n",
            "Iteration 880, loss = 0.00013692\n",
            "Iteration 881, loss = 0.00013674\n",
            "Iteration 882, loss = 0.00013585\n",
            "Iteration 883, loss = 0.00013562\n",
            "Iteration 884, loss = 0.00013527\n",
            "Iteration 885, loss = 0.00013669\n",
            "Iteration 886, loss = 0.00013604\n",
            "Iteration 887, loss = 0.00013568\n",
            "Iteration 888, loss = 0.00013475\n",
            "Iteration 889, loss = 0.00013475\n",
            "Iteration 890, loss = 0.00013494\n",
            "Iteration 891, loss = 0.00013484\n",
            "Iteration 892, loss = 0.00013484\n",
            "Iteration 893, loss = 0.00013436\n",
            "Iteration 894, loss = 0.00013411\n",
            "Iteration 895, loss = 0.00013393\n",
            "Iteration 896, loss = 0.00013421\n",
            "Iteration 897, loss = 0.00013368\n",
            "Iteration 898, loss = 0.00013387\n",
            "Iteration 899, loss = 0.00013369\n",
            "Iteration 900, loss = 0.00013425\n",
            "Iteration 901, loss = 0.00013426\n",
            "Iteration 902, loss = 0.00013308\n",
            "Iteration 903, loss = 0.00013301\n",
            "Iteration 904, loss = 0.00013302\n",
            "Iteration 905, loss = 0.00013380\n",
            "Iteration 906, loss = 0.00013363\n",
            "Iteration 907, loss = 0.00013269\n",
            "Iteration 908, loss = 0.00013287\n",
            "Iteration 909, loss = 0.00013317\n",
            "Iteration 910, loss = 0.00013285\n",
            "Iteration 911, loss = 0.00013292\n",
            "Iteration 912, loss = 0.00013204\n",
            "Iteration 913, loss = 0.00013168\n",
            "Iteration 914, loss = 0.00013205\n",
            "Iteration 915, loss = 0.00013348\n",
            "Iteration 916, loss = 0.00013177\n",
            "Iteration 917, loss = 0.00013175\n",
            "Iteration 918, loss = 0.00013131\n",
            "Iteration 919, loss = 0.00013174\n",
            "Iteration 920, loss = 0.00013189\n",
            "Iteration 921, loss = 0.00013072\n",
            "Iteration 922, loss = 0.00013136\n",
            "Iteration 923, loss = 0.00013096\n",
            "Iteration 924, loss = 0.00013026\n",
            "Iteration 925, loss = 0.00013053\n",
            "Iteration 926, loss = 0.00013072\n",
            "Iteration 927, loss = 0.00013059\n",
            "Iteration 928, loss = 0.00013029\n",
            "Iteration 929, loss = 0.00012976\n",
            "Iteration 930, loss = 0.00012970\n",
            "Iteration 931, loss = 0.00012982\n",
            "Iteration 932, loss = 0.00013002\n",
            "Iteration 933, loss = 0.00013233\n",
            "Iteration 934, loss = 0.00013149\n",
            "Iteration 935, loss = 0.00013005\n",
            "Iteration 936, loss = 0.00012890\n",
            "Iteration 937, loss = 0.00012903\n",
            "Iteration 938, loss = 0.00012883\n",
            "Iteration 939, loss = 0.00012878\n",
            "Iteration 940, loss = 0.00012858\n",
            "Iteration 941, loss = 0.00012837\n",
            "Iteration 942, loss = 0.00012835\n",
            "Iteration 943, loss = 0.00012832\n",
            "Iteration 944, loss = 0.00012842\n",
            "Iteration 945, loss = 0.00012847\n",
            "Iteration 946, loss = 0.00012967\n",
            "Iteration 947, loss = 0.00012929\n",
            "Iteration 948, loss = 0.00012979\n",
            "Iteration 949, loss = 0.00013056\n",
            "Iteration 950, loss = 0.00013025\n",
            "Iteration 951, loss = 0.00012822\n",
            "Iteration 952, loss = 0.00012955\n",
            "Iteration 953, loss = 0.00012838\n",
            "Iteration 954, loss = 0.00012793\n",
            "Iteration 955, loss = 0.00012741\n",
            "Iteration 956, loss = 0.00012686\n",
            "Iteration 957, loss = 0.00012723\n",
            "Iteration 958, loss = 0.00012702\n",
            "Iteration 959, loss = 0.00012725\n",
            "Iteration 960, loss = 0.00012654\n",
            "Iteration 961, loss = 0.00012630\n",
            "Iteration 962, loss = 0.00012623\n",
            "Iteration 963, loss = 0.00012607\n",
            "Iteration 964, loss = 0.00012676\n",
            "Iteration 965, loss = 0.00012737\n",
            "Iteration 966, loss = 0.00012663\n",
            "Iteration 967, loss = 0.00012588\n",
            "Iteration 968, loss = 0.00012583\n",
            "Iteration 969, loss = 0.00012600\n",
            "Iteration 970, loss = 0.00012572\n",
            "Iteration 971, loss = 0.00012563\n",
            "Iteration 972, loss = 0.00012605\n",
            "Iteration 973, loss = 0.00012650\n",
            "Iteration 974, loss = 0.00012603\n",
            "Iteration 975, loss = 0.00012551\n",
            "Iteration 976, loss = 0.00012507\n",
            "Iteration 977, loss = 0.00012500\n",
            "Iteration 978, loss = 0.00012459\n",
            "Iteration 979, loss = 0.00012451\n",
            "Iteration 980, loss = 0.00012499\n",
            "Iteration 981, loss = 0.00012485\n",
            "Iteration 982, loss = 0.00012437\n",
            "Iteration 983, loss = 0.00012425\n",
            "Iteration 984, loss = 0.00012416\n",
            "Iteration 985, loss = 0.00012397\n",
            "Iteration 986, loss = 0.00012392\n",
            "Iteration 987, loss = 0.00012393\n",
            "Iteration 988, loss = 0.00012380\n",
            "Iteration 989, loss = 0.00012393\n",
            "Iteration 990, loss = 0.00012368\n",
            "Iteration 991, loss = 0.00012347\n",
            "Iteration 992, loss = 0.00012367\n",
            "Iteration 993, loss = 0.00012354\n",
            "Iteration 994, loss = 0.00012327\n",
            "Iteration 995, loss = 0.00012319\n",
            "Iteration 996, loss = 0.00012322\n",
            "Iteration 997, loss = 0.00012333\n",
            "Iteration 998, loss = 0.00012422\n",
            "Iteration 999, loss = 0.00012403\n",
            "Iteration 1000, loss = 0.00012317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JleM6fQZ5YeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0731e470-487c-4c52-cc07-24885de56a19"
      },
      "source": [
        "nnModelADAM"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(90,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=1, shuffle=True, solver='adam',\n",
              "              tol=1e-19, validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E9h-DvK5gdW"
      },
      "source": [
        "nnModelLBFGS = mlpLBFGS.fit(X_train,y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULCS71e56a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841ac36f-9072-432d-e57b-9312e461ba41"
      },
      "source": [
        "nnModelLBFGS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(90,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
              "              tol=1e-19, validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dgNGarlQa5X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCuEwpQhQa5c"
      },
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CmfY2QmQa5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd4826d-590a-471a-ee0d-4870f8c02e3f"
      },
      "source": [
        "lr_model.score(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J2MetRaQa5z"
      },
      "source": [
        "# logistic regression : 98.4%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5upavseQa54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d6c8bf-5e0c-4756-fc47-a334e7afeff6"
      },
      "source": [
        "#PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca= PCA(n_components=200)\n",
        "pca.fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_model.score(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQHkRdxLQa5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fb1eea-4d21-4c2c-dd16-b9fa3baa41fe"
      },
      "source": [
        "# Feature scaling between -1 to 1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train= scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_model.score(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9HKPiQ11Exy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4e7GHDk1Ewc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYG21jc11Et-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMhiW95ZQa6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7484ecde-51bc-4234-f6af-58d6bdfa8cb9"
      },
      "source": [
        "#Random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train= scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=500)\n",
        "\n",
        "rfc.fit(X_train, y_train)\n",
        "rfc.score(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.948"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7_A-w7QQa6J"
      },
      "source": [
        "#Random forest : 94.8%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwYGprDiQa6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a73e40-9d8e-4578-d49e-e7258cb1e7c9"
      },
      "source": [
        "# KNN\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(n_neighbors=24)\n",
        "\n",
        "knnmodel= clf.fit(X_train, y_train)\n",
        "y_pred_knn= clf.predict(X_test)\n",
        "\n",
        "acc= accuracy_score(y_test, y_pred_knn)\n",
        "print(\"KNN accuracy : %.2f\" % (acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN accuracy : 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyuAqJhKQa6b"
      },
      "source": [
        "#KNN accuracy : 0.61"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfn_4Hr1Qa6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d429a74-4a93-4d5a-d326-9e3ac32b160c"
      },
      "source": [
        "#Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dtc= DecisionTreeClassifier()\n",
        "tree = dtc.fit(X_train,y_train)\n",
        "y_pred_dtc = tree.predict(X_test)\n",
        "\n",
        "acc1= accuracy_score(y_test,y_pred_dtc)\n",
        "print('Accuracy : %f'% acc1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.832000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJSS3eCaQa6j"
      },
      "source": [
        "# Decision Tree Accuracy : 0.832000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZJwAHBbQa6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4625d083-1ca1-4850-ff25-7a4aae4931f7"
      },
      "source": [
        "# Grid Search CV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "parameters = {\n",
        "    'kernel': ['linear', 'rbf', 'poly','sigmoid'],\n",
        "    'C': [100, 50, 20, 1, 0.1]\n",
        "}\n",
        "\n",
        "selector = GridSearchCV(SVC(), parameters, scoring='accuracy') # we only care about accuracy here\n",
        "selector.fit(X_train,y_train)\n",
        "\n",
        "print('Best parameter set found:')\n",
        "print(selector.best_params_)\n",
        "print('Detailed grid scores:')\n",
        "means = selector.cv_results_['mean_test_score']\n",
        "stds = selector.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, selector.cv_results_['params']):\n",
        "    print('%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameter set found:\n",
            "{'C': 1, 'kernel': 'sigmoid'}\n",
            "Detailed grid scores:\n",
            "0.963 (+/-0.011) for {'C': 100, 'kernel': 'linear'}\n",
            "\n",
            "0.948 (+/-0.019) for {'C': 100, 'kernel': 'rbf'}\n",
            "\n",
            "0.526 (+/-0.051) for {'C': 100, 'kernel': 'poly'}\n",
            "\n",
            "0.823 (+/-0.055) for {'C': 100, 'kernel': 'sigmoid'}\n",
            "\n",
            "0.963 (+/-0.011) for {'C': 50, 'kernel': 'linear'}\n",
            "\n",
            "0.948 (+/-0.019) for {'C': 50, 'kernel': 'rbf'}\n",
            "\n",
            "0.526 (+/-0.051) for {'C': 50, 'kernel': 'poly'}\n",
            "\n",
            "0.834 (+/-0.032) for {'C': 50, 'kernel': 'sigmoid'}\n",
            "\n",
            "0.963 (+/-0.011) for {'C': 20, 'kernel': 'linear'}\n",
            "\n",
            "0.948 (+/-0.019) for {'C': 20, 'kernel': 'rbf'}\n",
            "\n",
            "0.526 (+/-0.051) for {'C': 20, 'kernel': 'poly'}\n",
            "\n",
            "0.884 (+/-0.029) for {'C': 20, 'kernel': 'sigmoid'}\n",
            "\n",
            "0.963 (+/-0.011) for {'C': 1, 'kernel': 'linear'}\n",
            "\n",
            "0.940 (+/-0.025) for {'C': 1, 'kernel': 'rbf'}\n",
            "\n",
            "0.356 (+/-0.070) for {'C': 1, 'kernel': 'poly'}\n",
            "\n",
            "0.969 (+/-0.025) for {'C': 1, 'kernel': 'sigmoid'}\n",
            "\n",
            "0.964 (+/-0.013) for {'C': 0.1, 'kernel': 'linear'}\n",
            "\n",
            "0.212 (+/-0.021) for {'C': 0.1, 'kernel': 'rbf'}\n",
            "\n",
            "0.189 (+/-0.003) for {'C': 0.1, 'kernel': 'poly'}\n",
            "\n",
            "0.573 (+/-0.049) for {'C': 0.1, 'kernel': 'sigmoid'}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGe9k2PJQa6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdc8399-5ad2-4e1c-a19a-b46b88348d16"
      },
      "source": [
        "clf=SVC(kernel='sigmoid', C=1)\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred_clf= clf.predict(X_test)\n",
        "print('Accuracy score:',accuracy_score(y_test, y_pred_clf))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDAGJIXRQa60"
      },
      "source": [
        "#Grid search : Accuracy score: 97.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPmdOOeYQa7D"
      },
      "source": [
        "# Classification models\n",
        "# logistic regression : 98.4 %\n",
        "# Random forest : 94.8 %\n",
        "# KNN accuracy : 61 %\n",
        "# Decision Tree Accuracy : 83.20 %\n",
        "# Grid search : Accuracy score: 97.2 %"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neY_HqJzQa7H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}